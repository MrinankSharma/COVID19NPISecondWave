{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Czechia - load local area information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cz_df = pd.read_csv('../../data/raw_data_w_sources/cz_cases_deaths.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cz_df = pd.read_csv('../../data/raw_data_w_sources/cz_cases_deaths.csv')\n",
    "cz_df['date'] = pd.to_datetime(cz_df['date'], dayfirst=True)\n",
    "cz_df = cz_df[~cz_df['LAU Unit'].isnull()]\n",
    "cz_df = cz_df.rename({'NUTS3 Unit': 'area'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cz_timeseries_df = cz_df.groupby(['area', 'date']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts3_area_info_list = []\n",
    "\n",
    "start_dates = ['2020-03-01', '2020-04-01','2020-05-01', '2020-06-01', '2020-07-01', '2020-08-01', '2020-09-01', '2020-10-01', '2020-11-01', '2020-12-01', '2021-01-01']\n",
    "col_names = ['MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC', 'JAN']\n",
    "for nuts3_area in cz_timeseries_df.index.unique(0):\n",
    "    nuts3_area_dict = {\n",
    "        'area': nuts3_area,\n",
    "    }\n",
    "    \n",
    "    for col_name, start_date in zip(col_names, start_dates):\n",
    "        nuts3_area_dict[f'{col_name}-cumcases'] = cz_timeseries_df.loc[nuts3_area].loc[start_date]['Infected']\n",
    "        nuts3_area_dict[f'{col_name}-cumdeaths'] = cz_timeseries_df.loc[nuts3_area].loc[start_date]['Deaths']\n",
    "    \n",
    "    nuts3_area_info_list.append(nuts3_area_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cz_area_df = pd.DataFrame(nuts3_area_info_list).set_index('area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cz_timeseries_df['new_cases'] = cz_timeseries_df.groupby(level=[0]).diff()['Infected']\n",
    "cz_timeseries_df['new_deaths'] = cz_timeseries_df.groupby(level=[0]).diff()['Deaths']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Switzerland - load local area information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swiss_df = pd.read_csv('../../data/raw_data_w_sources/ch_cases_deaths.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swiss_df = swiss_df.drop(swiss_df.columns.difference(['date', 'abbreviation_canton_and_fl', 'ncumul_conf', 'ncumul_deceased']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swiss_df = swiss_df.rename({'abbreviation_canton_and_fl': 'area'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../../data/raw_data_w_sources/ch_canton_lookup.json', 'r') as fp:\n",
    "    swiss_canton_lookup = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range('2020-03-01', '2021-01-01')\n",
    "swiss_df['date'] = pd.to_datetime(swiss_df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_swiss_df = swiss_df.set_index('date').groupby('area').apply(lambda x: x.reindex(dates, fill_value=None).drop('area', axis=1)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swiss_ts_df = filled_swiss_df.replace(swiss_canton_lookup)\n",
    "swiss_ts_df = swiss_ts_df.rename({'level_1': 'date'}, axis=1)\n",
    "swiss_ts_df = swiss_ts_df.set_index(['area', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for canton in swiss_ts_df.index.unique(0):\n",
    "    if np.isnan(swiss_ts_df.loc[canton].loc[dates[0]]['ncumul_conf']):\n",
    "        swiss_ts_df.loc[(canton, dates[0]), 'ncumul_conf'] = 0\n",
    "    \n",
    "    if np.isnan(swiss_ts_df.loc[canton].loc[dates[0]]['ncumul_deceased']):\n",
    "        swiss_ts_df.loc[(canton, dates[0]), 'ncumul_deceased'] = 0\n",
    "    \n",
    "    interp_df = swiss_ts_df.loc[canton].interpolate()\n",
    "    interp_df['new_cases'] = interp_df['ncumul_conf'].diff()\n",
    "    interp_df['new_deaths'] = interp_df['ncumul_deceased'].diff()\n",
    "    for date in dates:\n",
    "        swiss_ts_df.loc[(canton, date), 'ncumul_conf'] = interp_df.loc[date, 'ncumul_conf']\n",
    "        swiss_ts_df.loc[(canton, date), 'ncumul_deceased'] = interp_df.loc[date, 'ncumul_deceased']\n",
    "        swiss_ts_df.loc[(canton, date), 'new_cases'] = np.around(interp_df.loc[date, 'new_cases'])\n",
    "        swiss_ts_df.loc[(canton, date), 'new_deaths'] = np.around(interp_df.loc[date, 'new_deaths'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warning: there was some missing data here\n",
    "\n",
    "e.g., in some areas, some days are missing. To get around this, I did linear interpolation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swiss_df = swiss_df.replace(swiss_canton_lookup)\n",
    "swiss_df = swiss_df.set_index(['area', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for canton in swiss_df.index.unique(0):\n",
    "    canton_df = swiss_df.loc[canton]\n",
    "    all_dates = pd.date_range(canton_df.index[0], canton_df.index[-1])\n",
    "    print(f'Canton: {canton} has {100*float(len(canton_df.index)/len(all_dates)):.2f}% of dates and {canton_df.loc[canton_df.index[-1]][\"ncumul_conf\"]} cases all pandemic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canton_info_list = []\n",
    "\n",
    "start_dates = ['2020-03-01', '2020-04-01','2020-05-01', '2020-06-01', '2020-07-01', '2020-08-01', '2020-09-01', '2020-10-01', '2020-11-01', '2020-12-01', '2021-01-01']\n",
    "col_names = ['MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC', 'JAN']\n",
    "for canton in swiss_df.index.unique(0):\n",
    "    canton_dict = {\n",
    "        'area': canton,\n",
    "    }\n",
    "    \n",
    "    for col_name, start_date in zip(col_names, start_dates):\n",
    "        canton_dict[f'{col_name}-cumcases'] = swiss_ts_df.loc[canton].loc[start_date]['ncumul_conf']\n",
    "        canton_dict[f'{col_name}-cumdeaths'] = swiss_ts_df.loc[canton].loc[start_date]['ncumul_deceased']\n",
    "    \n",
    "    canton_info_list.append(canton_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swiss_area_df = pd.DataFrame(canton_info_list).set_index('area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swiss_ts_df = swiss_ts_df.drop(['ncumul_conf', 'ncumul_deceased'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swiss_timeseries_df = swiss_ts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Germany - load local area information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ags dict contains information about the local areas of germany\n",
    "with open('../../data/raw_data_w_sources/de_ags.json') as json_file:\n",
    "    ags_info_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_df = pd.read_csv('../../data/raw_data_w_sources/de_cases-rki-by-ags.csv')\n",
    "cases_df = cases_df.drop('sum_cases', axis=1)\n",
    "cases_df = cases_df.rename({'time_iso8601': 'date'}, axis=1)\n",
    "cases_df['date'] = pd.to_datetime(cases_df['date'])\n",
    "cases_df['date'] = pd.to_datetime(cases_df['date'].dt.date)\n",
    "cases_df = cases_df.set_index('date')\n",
    "\n",
    "deaths_df = pd.read_csv('../../data/raw_data_w_sources/de_deaths-rki-by-ags.csv')\n",
    "deaths = deaths_df.drop('sum_deaths', axis=1)\n",
    "deaths_df = deaths_df.rename({'time_iso8601': 'date'}, axis=1)\n",
    "deaths_df['date'] = pd.to_datetime(deaths_df['date'])\n",
    "deaths_df['date'] = pd.to_datetime(deaths_df['date'].dt.date)\n",
    "deaths_df = deaths_df.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ags_info_list = []\n",
    "\n",
    "start_dates = ['2020-03-02', '2020-04-01','2020-05-01', '2020-06-01', '2020-07-01', '2020-08-01', '2020-09-01', '2020-10-01', '2020-11-01', '2020-12-01', '2021-01-01']\n",
    "col_names = ['MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC', 'JAN']\n",
    "for ags in ags_info_dict.keys():\n",
    "    if ags == '3152':\n",
    "        continue\n",
    "    \n",
    "    ags_dict = {\n",
    "        'area': ags_info_dict[ags]['name'],\n",
    "        'region': ags_info_dict[ags]['state'],\n",
    "        'country': 'Germany',\n",
    "    }\n",
    "    for col_name, start_date in zip(col_names, start_dates):\n",
    "        ags_dict[f'{col_name}-cumcases'] = cases_df[ags].loc[start_date]\n",
    "        ags_dict[f'{col_name}-cumdeaths'] = deaths_df[ags].loc[start_date]\n",
    "    \n",
    "    ags_info_list.append(ags_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_df = pd.read_csv('../../data/raw_data_w_sources/de_cases-rki-by-ags.csv')\n",
    "cases_df = cases_df.drop('sum_cases', axis=1)\n",
    "cases_df = cases_df.rename({'time_iso8601': 'date'}, axis=1)\n",
    "cases_df['date'] = pd.to_datetime(cases_df['date'])\n",
    "cases_df['date'] = pd.to_datetime(cases_df['date'].dt.date)\n",
    "cases_df = cases_df.set_index('date')\n",
    "cases_df = cases_df.diff()\n",
    "\n",
    "deaths_df = pd.read_csv('../../data/raw_data_w_sources/de_deaths-rki-by-ags.csv')\n",
    "deaths = deaths_df.drop('sum_deaths', axis=1)\n",
    "deaths_df = deaths_df.rename({'time_iso8601': 'date'}, axis=1)\n",
    "deaths_df['date'] = pd.to_datetime(deaths_df['date'])\n",
    "deaths_df['date'] = pd.to_datetime(deaths_df['date'].dt.date)\n",
    "deaths_df = deaths_df.set_index('date')\n",
    "deaths_df = cases_df.diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ags_time_series_list = []\n",
    "\n",
    "Ds = pd.date_range('2020-03-02', '2021-01-01')\n",
    "for ags in ags_info_dict.keys():\n",
    "    if ags == '3152':\n",
    "        continue\n",
    "        \n",
    "    for d in Ds:\n",
    "        ags_dict = {\n",
    "            'area': ags_info_dict[ags]['name'],\n",
    "            'date': d\n",
    "        }\n",
    "        ags_dict['new_cases'] = cases_df[ags][d]\n",
    "        ags_dict['new_deaths'] = deaths_df[ags][d]\n",
    "        \n",
    "        ags_time_series_list.append(ags_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "germany_ts_df = pd.DataFrame(ags_time_series_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "germany_area_df = pd.DataFrame(ags_info_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "germany_area_df = germany_area_df.set_index('area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "germany_ts_df = germany_ts_df.set_index(['area', 'date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UK - load ltla df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: for the UK, at the moment, we don't have region info for the LTLAs in Northern Ireland, Scotland, or Wales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/raw_data_w_sources/uk_ltla_info.json') as json_file:\n",
    "    uk_ltla_info_dict = json.load(json_file)\n",
    "\n",
    "uk_ltla_info_df = pd.DataFrame([d['attributes'] for d in uk_ltla_info_dict['features']])\n",
    "uk_ltla_info_df = uk_ltla_info_df.rename({'LAU117NM': 'area', 'NUTS318NM': 'NUTS3', 'NUTS118NM': 'region'} ,axis=1)\n",
    "uk_ltla_info_df = uk_ltla_info_df.set_index('area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_df = pd.read_csv('../../data/raw_data_w_sources/uk_case_deaths.csv', infer_datetime_format=True)\n",
    "uk_df = uk_df.drop(['areaCode', 'newCasesByPublishDate', 'newDeaths28DaysByPublishDate'], axis=1)\n",
    "uk_df['areaType'] = 'UK'\n",
    "uk_df = uk_df.rename({'areaType': 'country', 'areaName':'area', 'newCasesBySpecimenDate': 'new_cases', 'newDeaths28DaysByDeathDate': 'new_deaths'}, axis=1)\n",
    "uk_df = uk_df.set_index(['area', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_df = uk_df.sort_index(level=[1],ascending=[True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_areas = uk_df.index.unique(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dates = ['2020-03-01', '2020-04-01','2020-05-01', '2020-06-01', '2020-07-01', '2020-08-01', '2020-09-01', '2020-10-01', '2020-11-01', '2020-12-01', '2021-01-01']\n",
    "col_names = ['MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC', 'JAN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_area_list = []\n",
    "\n",
    "for ltla in uk_areas:\n",
    "\n",
    "    try:\n",
    "        region = uk_ltla_info_df.loc[ltla]['region']\n",
    "        nuts3 = uk_ltla_info_df.loc[ltla]['NUTS3']\n",
    "    except KeyError:\n",
    "        print(f'{ltla} missing in my lookup table')\n",
    "        region = 'unknown'\n",
    "        nuts3 = 'unknown'\n",
    "        \n",
    "    \n",
    "    ltla_dict = {\n",
    "        'name': ltla,\n",
    "        'region':  region,\n",
    "        'NUTS3':  nuts3,\n",
    "        'country': 'UK',\n",
    "    }\n",
    "    \n",
    "    cum_cases = uk_df.loc[ltla]['new_cases'].cumsum()\n",
    "    cum_deaths = uk_df.loc[ltla]['new_deaths'].cumsum()\n",
    "    for col_name, start_date in zip(col_names, start_dates):\n",
    "        ltla_dict[f'{col_name}-cumcases'] = cum_cases[start_date]\n",
    "        ltla_dict[f'{col_name}-cumdeaths'] = cum_deaths[start_date]\n",
    "    \n",
    "    uk_area_list.append(ltla_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_area_df = pd.DataFrame(uk_area_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UK LTLA converted to NUTS3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NUTS3_lookup(ltla):\n",
    "    try:\n",
    "        nuts3 = uk_ltla_info_df.loc[ltla]['NUTS3']\n",
    "    except KeyError:\n",
    "#         print(f'{ltla} missing in my lookup table')\n",
    "        nuts3 = 'unknown'\n",
    "    return nuts3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts3_uk_df = uk_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts3_uk_df['NUTS3'] = nuts3_uk_df['area'].map(NUTS3_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = nuts3_uk_df['date'].unique()\n",
    "nuts3_regions = nuts3_uk_df['NUTS3'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts3_df_list = []\n",
    "\n",
    "nuts3_uk_df_merged = None\n",
    "\n",
    "for nuts3_region in nuts3_regions:\n",
    "    if nuts3_region == 'unknown':\n",
    "        continue\n",
    "    \n",
    "    filtered_df = nuts3_uk_df.loc[nuts3_uk_df['NUTS3'] == nuts3_region]\n",
    "    \n",
    "    case_death_series = filtered_df.groupby('date').sum()\n",
    "    case_death_series['area'] = nuts3_region\n",
    "    \n",
    "    if nuts3_uk_df_merged is None:\n",
    "        nuts3_uk_df_merged = copy.deepcopy(case_death_series)\n",
    "    else:\n",
    "        nuts3_uk_df_merged = nuts3_uk_df_merged.append(case_death_series)\n",
    "    \n",
    "nuts3_uk_df_merged = nuts3_uk_df_merged.reset_index()\n",
    "nuts3_uk_df_merged['date'] = pd.to_datetime(nuts3_uk_df_merged['date'])\n",
    "nuts3_uk_df_merged = nuts3_uk_df_merged.set_index(['area', 'date'])\n",
    "nuts3_uk_df_merged = nuts3_uk_df_merged.sort_index(level=[1],ascending=[True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_nuts3_area_list = []\n",
    "\n",
    "for nuts3_region in nuts3_regions:\n",
    "    if nuts3_region == 'unknown':\n",
    "        continue\n",
    "            \n",
    "    nuts3_dict = {\n",
    "        'area': nuts3_region,\n",
    "        'region': uk_ltla_info_df.loc[uk_ltla_info_df['NUTS3'] == nuts3_region]['region'][0]\n",
    "    }\n",
    "    cum_cases = nuts3_uk_df_merged.loc[nuts3_region]['new_cases'].cumsum()\n",
    "    cum_deaths = nuts3_uk_df_merged.loc[nuts3_region]['new_deaths'].cumsum()\n",
    "    \n",
    "    for col_name, start_date in zip(col_names, start_dates):\n",
    "        nuts3_dict[f'{col_name}-cumcases'] = cum_cases[start_date]\n",
    "        nuts3_dict[f'{col_name}-cumdeaths'] = cum_deaths[start_date]\n",
    "    \n",
    "    uk_nuts3_area_list.append(nuts3_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_nuts3_area_df = pd.DataFrame(uk_nuts3_area_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_timeseries_df = nuts3_uk_df_merged\n",
    "uk_area_df = uk_nuts3_area_df.set_index('area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_area_df = uk_area_df.loc[uk_area_df['region'] != 'Scotland']\n",
    "uk_area_df = uk_area_df.loc[uk_area_df['region'] != 'Wales']\n",
    "uk_area_df = uk_area_df.loc[uk_area_df['region'] != 'Northern Ireland']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Austria - load local area dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austria_ltla_lookup = pd.read_csv('../../data/raw_data_w_sources/at_lau_lookup.csv')\n",
    "austria_ltla_lookup = austria_ltla_lookup.set_index('GKZ')\n",
    "\n",
    "def at_ltla_lookup(ltla):\n",
    "    if ltla in austria_ltla_lookup.index:\n",
    "        return austria_ltla_lookup.loc[ltla]['State Code (middle column of HASC)']\n",
    "    return 'Vienna'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austria_df = pd.read_csv('../../data/raw_data_w_sources/at_case_deaths.csv', error_bad_lines=False, delimiter=';', skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austria_df = pd.read_csv('../../data/raw_data_w_sources/at_case_deaths.csv', error_bad_lines=False, delimiter=';', skiprows=1)\n",
    "austria_df = austria_df.drop([' number of cases total',\n",
    "       ' number of cases of 7 days', ' seven days of incidence cases',' number of total totals',\n",
    "       ' number of held daily', ' number of healing total'], axis=1)\n",
    "austria_df[' GKZ'] = austria_df[' GKZ'].map(at_ltla_lookup)\n",
    "austria_df = austria_df.rename({'Time': 'date', ' district': 'area', ' GKZ': 'region', ' number of inhabitants': 'population', ' number of cases': 'new_cases', ' number of dead daily': 'new_deaths'}, axis=1)\n",
    "austria_df = austria_df.drop('population', axis=1)\n",
    "austria_df['date'] = pd.to_datetime(austria_df['date'], format='%d.%m.%Y %M:%H:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austria_df = austria_df.set_index(['area', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aut_area_list = []\n",
    "start_dates = ['2020-03-01', '2020-04-01','2020-05-01', '2020-06-01', '2020-07-01', '2020-08-01', '2020-09-01', '2020-10-01', '2020-11-01', '2020-12-01', '2021-01-01']\n",
    "col_names = ['MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC', 'JAN']\n",
    "\n",
    "start_dates = ['2020-03-01', '2020-04-01','2020-05-01', '2020-06-01', '2020-07-01', '2020-08-01', '2020-09-01', '2020-10-01', '2020-11-01', '2020-12-01', '2021-01-01']\n",
    "col_names = ['MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC', 'JAN']\n",
    "\n",
    "\n",
    "for area in austria_df.index.unique(0):\n",
    "    area_dict = {\n",
    "        'area': area,\n",
    "        'region': austria_df.loc[area].iloc[0]['region'],\n",
    "        'country': 'Austria',\n",
    "    }\n",
    "    \n",
    "    cum_cases = austria_df.loc[area]['new_cases'].cumsum()\n",
    "    cum_deaths = austria_df.loc[area]['new_deaths'].cumsum()\n",
    "    for col_name, start_date in zip(col_names, start_dates):\n",
    "        area_dict[f'{col_name}-cumcases'] = cum_cases[start_date]\n",
    "        area_dict[f'{col_name}-cumdeaths'] = cum_deaths[start_date]\n",
    "    \n",
    "    aut_area_list.append(area_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austria_timeseries_df = austria_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austria_area_df = pd.DataFrame(aut_area_list).set_index('area')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Austria - but done at a higher level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austria_nuts2_regions = austria_timeseries_df['region'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austria_nuts2_df_list = []\n",
    "\n",
    "austria_nuts2_df_merged = None\n",
    "\n",
    "for nuts2_region in austria_nuts2_regions:    \n",
    "    filtered_df = austria_timeseries_df.loc[austria_timeseries_df['region'] == nuts2_region]\n",
    "    \n",
    "    case_death_series = filtered_df.groupby('date').sum()\n",
    "    case_death_series['area'] = nuts2_region\n",
    "    \n",
    "    if austria_nuts2_df_merged is None:\n",
    "        austria_nuts2_df_merged = copy.deepcopy(case_death_series)\n",
    "    else:\n",
    "        austria_nuts2_df_merged = austria_nuts2_df_merged.append(case_death_series)\n",
    "    \n",
    "austria_nuts2_df_merged = austria_nuts2_df_merged.reset_index()\n",
    "austria_nuts2_df_merged = austria_nuts2_df_merged.set_index(['area', 'date'])\n",
    "austria_nuts2_df_merged = austria_nuts2_df_merged.sort_index(level=[1],ascending=[True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austria_nuts2_timeseries_df = austria_nuts2_df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austria_nuts2_area_list = []\n",
    "\n",
    "for nuts2_region in austria_nuts2_regions:\n",
    "    nuts2_dict = {\n",
    "        'area': nuts2_region\n",
    "    }\n",
    "    \n",
    "    cum_cases = austria_nuts2_timeseries_df.loc[nuts2_region]['new_cases'].cumsum()\n",
    "    cum_deaths = austria_nuts2_timeseries_df.loc[nuts2_region]['new_deaths'].cumsum()\n",
    "    \n",
    "    for col_name, start_date in zip(col_names, start_dates):\n",
    "        nuts2_dict[f'{col_name}-cumcases'] = cum_cases[start_date]\n",
    "        nuts2_dict[f'{col_name}-cumdeaths'] = cum_deaths[start_date]\n",
    "    \n",
    "    austria_nuts2_area_list.append(nuts2_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austria_nuts2_area_df = pd.DataFrame(austria_nuts2_area_list)\n",
    "austria_nuts2_area_df = austria_nuts2_area_df.set_index('area')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Italy Case and Death Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italy_df = pd.read_csv('../../data/raw_data_w_sources/it_cases_deaths.csv', delimiter=',')\n",
    "italy_df['date'] = pd.to_datetime(italy_df['date'])\n",
    "italy_df['date'] = italy_df['date'].dt.date\n",
    "italy_df = italy_df.set_index(['area', 'date'])\n",
    "italy_df['new_deaths'] = italy_df.groupby('area').diff()['total_deaths']\n",
    "italy_df = italy_df.drop('total_deaths', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italy_timeseries_df = italy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italy_area_list = []\n",
    "start_dates = ['2020-03-01', '2020-04-01','2020-05-01', '2020-06-01', '2020-07-01', '2020-08-01', '2020-09-01', '2020-10-01', '2020-11-01', '2020-12-01', '2021-01-01']\n",
    "col_names = ['MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC', 'JAN']\n",
    "\n",
    "start_dates = ['2020-03-01', '2020-04-01','2020-05-01', '2020-06-01', '2020-07-01', '2020-08-01', '2020-09-01', '2020-10-01', '2020-11-01', '2020-12-01', '2021-01-01']\n",
    "col_names = ['MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC', 'JAN']\n",
    "\n",
    "\n",
    "for area in italy_timeseries_df.index.unique(0):\n",
    "    area_dict = {\n",
    "        'area': area,\n",
    "    }\n",
    "    \n",
    "    cum_cases = italy_timeseries_df.loc[area]['new_cases'].cumsum()\n",
    "    cum_deaths = italy_timeseries_df.loc[area]['new_deaths'].cumsum()\n",
    "    for col_name, start_date in zip(col_names, start_dates):\n",
    "        area_dict[f'{col_name}-cumcases'] = cum_cases[start_date]\n",
    "        area_dict[f'{col_name}-cumdeaths'] = cum_deaths[start_date]\n",
    "    \n",
    "    italy_area_list.append(area_dict)\n",
    "\n",
    "italy_area_df = pd.DataFrame(italy_area_list).set_index('area')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actually Perform Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def threshold_cumulative_cases(area_df, threshold = 200, start_str='AUG', end_str='JAN'):\n",
    "    total_cases = area_df[f'{end_str}-cumcases'] - area_df[f'{start_str}-cumcases']\n",
    "    filtered_df = total_cases[total_cases>threshold].to_frame(f'cumulative_cases_{start_str}_{end_str}')\n",
    "    filtered_df = filtered_df.sort_values(filtered_df.columns[0])\n",
    "    \n",
    "    return filtered_df, 100*len(filtered_df.index)/len(area_df.index)\n",
    "\n",
    "def verify_timeseries(filtered_df, timeseries_df, plot_title='', n_regions=5):\n",
    "    areas_to_plot = filtered_df.index[:n_regions]\n",
    "    \n",
    "    plt.figure(figsize=(8, 8), dpi=300)\n",
    "    \n",
    "    Ds = pd.date_range('2020-08-01', '2020-12-30')\n",
    "    for p_i, area in enumerate(areas_to_plot):\n",
    "        plt.subplot(n_regions, 1, p_i+1)\n",
    "        plt.plot(timeseries_df.loc[area].loc[Ds]['new_cases'], color='tab:blue')\n",
    "        plt.ylabel('cases')\n",
    "        plt.yscale('log')\n",
    "        plt.ylim([10**0.5, 10**3])\n",
    "        plt.twinx()\n",
    "        plt.plot(timeseries_df.loc[area].loc[Ds]['new_deaths'], color='tab:red')\n",
    "        plt.title(f'{area}')\n",
    "        plt.yscale('log')\n",
    "        plt.ylabel('deaths')\n",
    "        plt.ylim([10**0.5, 10**2])\n",
    "    \n",
    "    plt.suptitle(plot_title)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df, prop_remaining = threshold_cumulative_cases(austria_nuts2_area_df, 10000)\n",
    "verify_timeseries(filtered_df, austria_nuts2_timeseries_df, 'Austria - NUTS2 Level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df, prop_remaining = threshold_cumulative_cases(italy_area_df, 4000)\n",
    "verify_timeseries(filtered_df, italy_timeseries_df, 'Italy - NUTS2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# how many areas remaining at different threshold vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_vals = np.linspace(100, 10000, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_dfs = [germany_ts_df, austria_timeseries_df, italy_timeseries_df, austria_nuts2_timeseries_df, uk_timeseries_df, swiss_timeseries_df]\n",
    "area_dfs = [germany_area_df, austria_area_df, italy_area_df, austria_nuts2_area_df, uk_area_df, swiss_area_df]\n",
    "titles = ['germany (nuts3)', 'austria (nuts3)', 'italy', 'austria (nuts2)', 'uk', 'swizterland']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3), dpi=200)\n",
    "for plot_i, (ts_df, area_df, title) in enumerate(zip(timeseries_dfs, area_dfs, titles)):\n",
    "    p_remaining = np.array([p for _, p in [threshold_cumulative_cases(area_df, t_val) for t_val in threshold_vals]])\n",
    "    plt.plot(threshold_vals, p_remaining, label=title)\n",
    "    plt.xlabel('case threshold')\n",
    "    plt.ylabel('percent areas remaining %')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_vals = [2000]\n",
    "timeseries_dfs = [germany_ts_df, austria_timeseries_df, swiss_timeseries_df]\n",
    "area_dfs = [germany_area_df, austria_area_df, swiss_area_df]\n",
    "titles = ['germany (nuts3)', 'austria (nuts3)', 'switzerland']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t_val in threshold_vals:\n",
    "    for ts_df, area_df, title in zip(timeseries_dfs, area_dfs, titles):\n",
    "        filtered_df, p = threshold_cumulative_cases(area_df, t_val)\n",
    "        verify_timeseries(filtered_df, ts_df, f'Thresholded {t_val}\\n{title}\\n{p}% of areas remaining')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement deaths stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining first wave as deaths from first march to end of june (start of july)\n",
    "def compute_first_wave_deaths(area_df, start_str='MAR', end_str='JUL'):\n",
    "    fw_deaths = area_df[f'{end_str}-cumdeaths'] - area_df[f'{start_str}-cumdeaths']\n",
    "    fw_deaths = fw_deaths.to_frame(f'first_wave_deaths')\n",
    "    filtered_df = fw_deaths.sort_values(fw_deaths.columns[0])\n",
    "    \n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_dfs = [germany_ts_df, austria_timeseries_df, italy_timeseries_df, austria_nuts2_timeseries_df, uk_timeseries_df, swiss_timeseries_df]\n",
    "area_dfs = [germany_area_df, austria_area_df, italy_area_df, austria_nuts2_area_df, uk_area_df, swiss_area_df]\n",
    "titles = ['germany (nuts3)', 'austria (nuts3)', 'italy', 'austria (nuts2)', 'uk', 'switzerland']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6), dpi=300)\n",
    "for plot_index, (area_df, title) in enumerate(zip(area_dfs, titles)):\n",
    "    plt.subplot(3, 2, plot_index+1)\n",
    "    sns.histplot(compute_first_wave_deaths(area_df))\n",
    "    plt.title(title)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italy_area_df.index.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_and_stratify(t_val, area_df, n_groups=5, samples_per_group=3, required=None):\n",
    "    filtered_df, p = threshold_cumulative_cases(area_df, t_val)\n",
    "    print(f'Under this threshold, there are {p}% ({len(filtered_df.index)}) areas remaining')\n",
    "    \n",
    "    full_thresholded_df = area_df.loc[filtered_df.index]\n",
    "    fw_deaths = compute_first_wave_deaths(full_thresholded_df)\n",
    "    fw_deaths = fw_deaths.sort_values('first_wave_deaths', ascending=False)\n",
    "\n",
    "    stratifications = np.linspace(0, 1, n_percentiles+1)\n",
    "    groups = np.array_split(list(fw_deaths.index), n_groups)\n",
    "    \n",
    "    samples_remaining = samples_per_group * np.ones(shape=5)\n",
    "    \n",
    "    samples = []\n",
    "    \n",
    "    if required is not None:\n",
    "        for r in required:\n",
    "            group = [i for i, group in enumerate(groups) if r in group][0]\n",
    "            samples.append(r)\n",
    "            samples_remaining[group] = samples_remaining[group] - 1\n",
    "    \n",
    "    for g, n_samples in zip(groups, samples_remaining):\n",
    "        if n_samples > 0:\n",
    "            samples.extend(np.random.choice(g, int(n_samples), replace=False).tolist())        \n",
    "            \n",
    "    return samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
