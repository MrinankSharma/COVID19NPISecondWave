{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Merging NPI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "import copy\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this dict contains information for loading the info\n",
    "npi_information_dict = {\n",
    "    'Public Outdoor Gathering Person Limit': {\n",
    "        'type': 'value',\n",
    "        'start_date_col': 'Start date (Public Outdoors)',\n",
    "        'end_date_col': 'End date (Public Outdoors)',\n",
    "        'value_col': 'Limit on number of people (only X people or fewer) (Public Outdoors)'\n",
    "    },\n",
    "    'Public Indoor Gathering Person Limit': {\n",
    "        'type': 'value',\n",
    "        'start_date_col': 'Start date (Public Indoors)',\n",
    "        'end_date_col': 'End date (Public Indoors)',\n",
    "        'value_col': 'Limit on number of people (only X people or fewer) (Public Indoors)'\n",
    "    },\n",
    "    'Private Outdoor Gathering Person Limit': {\n",
    "        'type': 'value',\n",
    "        'start_date_col': 'Start date (Private Outdoors)',\n",
    "        'end_date_col': 'End date (Private Outdoors)',\n",
    "        'value_col': 'Limit on number of people (only X people or fewer) (Private Outdoors)'\n",
    "    },\n",
    "    'Private Indoor Gathering Person Limit': {\n",
    "        'type': 'value',\n",
    "        'start_date_col': 'Start date (Private Indoors)',\n",
    "        'end_date_col': 'End date (Private Indoors)',\n",
    "        'value_col': 'Limit on number of people (only X people or fewer) (Private Indoors)'\n",
    "    },\n",
    "    'Public Outdoor Household Limit': {\n",
    "        'type': 'value',\n",
    "        'start_date_col': 'Start date (Public Outdoors)',\n",
    "        'end_date_col': 'End date (Public Outdoors)',\n",
    "        'value_col': 'Limit on number of households (Public Outdoors)'\n",
    "    },\n",
    "    'Public Indoor Household Limit': {\n",
    "        'type': 'value',\n",
    "        'start_date_col': 'Start date (Public Indoors)',\n",
    "        'end_date_col': 'End date (Public Indoors)',\n",
    "        'value_col': 'Limit on number of households (Public Indoors)'\n",
    "    },\n",
    "    'Private Outdoor Household Limit': {\n",
    "        'type': 'value',\n",
    "        'start_date_col': 'Start date (Private Outdoors)',\n",
    "        'end_date_col': 'End date (Private Outdoors)',\n",
    "        'value_col': 'Limit on number of households (Private Outdoors)'\n",
    "    },\n",
    "    'Private Indoor Household Limit': {\n",
    "        'type': 'value',\n",
    "        'start_date_col': 'Start date (Private Indoors)',\n",
    "        'end_date_col': 'End date (Private Indoors)',\n",
    "        'value_col': 'Limit on number of households (Private Indoors)'\n",
    "    },\n",
    "    'Mandatory Mask Wearing': {\n",
    "        'type': 'value',\n",
    "        'start_date_col': 'Start date (Mask Wearing)',\n",
    "        'end_date_col': 'End date (Mask Wearing)',\n",
    "        'value_col': 'Level of NPI (0-4) (Mask Wearing)'\n",
    "    },\n",
    "    'Some Face-to-Face Businesses Closed': {\n",
    "        'type': 'binary',\n",
    "        'start_date_col': 'Start date (Some Face2Face)',\n",
    "        'end_date_col': 'End date (Some Face2Face)'\n",
    "    },\n",
    "    'Gastronomy Closed': {\n",
    "        'type': 'binary',\n",
    "        'start_date_col': 'Start date  (Gastronomy Closed)',\n",
    "        'end_date_col': 'End date (Gastronomy Closed)'\n",
    "    },\n",
    "    'Leisure Venues Closed': {\n",
    "        'type': 'binary',\n",
    "        'start_date_col': 'Start date (Leisure Venue)',\n",
    "        'end_date_col': 'End date (Leisure Venue)'\n",
    "    },\n",
    "    'Retail Closed': {\n",
    "        'type': 'binary',\n",
    "        'start_date_col': 'Start date (Retail)',\n",
    "        'end_date_col': 'End date (Retail)'\n",
    "    },\n",
    "    'All Face-to-Face Businesses Closed': {\n",
    "        'type': 'binary',\n",
    "        'start_date_col': 'Start date (All Face2Face)',\n",
    "        'end_date_col': 'End date (All Face2Face)'\n",
    "    },\n",
    "    'Stay at Home Order': {\n",
    "        'type': 'binary',\n",
    "        'start_date_col': 'Start date (Stay Home)',\n",
    "        'end_date_col': 'End date (Stay Home)'\n",
    "    },\n",
    "    'Curfew': {\n",
    "        'type': 'binary',\n",
    "        'start_date_col': 'Start date (Curfew)',\n",
    "        'end_date_col': 'End date (Curfew)'\n",
    "    },\n",
    "    'Childcare Closed': {\n",
    "        'type': 'binary',\n",
    "        'start_date_col': 'Start date (Childcare)',\n",
    "        'end_date_col': 'End date  (Childcare)'\n",
    "    },\n",
    "    'Primary Schools Closed': {\n",
    "        'type': 'binary',\n",
    "        'start_date_col': 'Start date (Primary Schools)',\n",
    "        'end_date_col': 'End date (Primary Schools)'\n",
    "    },\n",
    "    'Secondary Schools Closed': {\n",
    "        'type': 'binary',\n",
    "        'start_date_col': 'Start date (Secondary Schools)',\n",
    "        'end_date_col': 'End date (Secondary Schools)'\n",
    "    },\n",
    "    'Universities Away': {\n",
    "        'type': 'binary',\n",
    "        'start_date_col': 'Start date (Unis Away)',\n",
    "        'end_date_col': 'End date (Unis Away)'\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there is a string, this lookup converts the month string into the month int and year int\n",
    "# not the cleanest way of doing this....\n",
    "lookup_months = {\n",
    "    'January': (1, 2021),\n",
    "    'Jan': (1, 2021),\n",
    "    'September': (9, 2020),\n",
    "    'Septemeber': (9, 2020),\n",
    "    'December': (12, 2020),\n",
    "    'August': (8, 2020),\n",
    "    'November': (11, 2020),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_start_date_str(start_date_str):\n",
    "    if start_date_str.strip() in ['Before 1 August', 'before 1 August 2020', 'Before 1 August 2020', 'Before August 1', '1 August 2020', 'Before 1st of August', 'before 01/08/2020', 'before 1/08/2020',  \n",
    "                                  'before 1/8/2020', 'before 13/7/2020', 'Before 1st August']:\n",
    "        return pd.to_datetime('2020-08-01')\n",
    "    \n",
    "    elif start_date_str.strip() in ['no', 'No', 'nan', 'N/A', 'NA']:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        return pd.to_datetime(start_date_str, dayfirst=True, infer_datetime_format=True)\n",
    "    except ValueError:\n",
    "        day = int(re.search('[0-9]+', start_date_str)[0])\n",
    "        for month_str, (m, y) in lookup_months.items():\n",
    "            if month_str in start_date_str:\n",
    "                dt = pd.Timestamp(day=day, month=m, year=y)\n",
    "                print(f'Regex Succeeded: Converted {start_date_str} to {dt}')\n",
    "                return dt\n",
    "        print(f'Could not convert start date {start_date_str}')\n",
    "        \n",
    "def process_end_date_str(end_date_str):\n",
    "    if end_date_str.strip() in ['no', 'No', 'nan', 'N/A', 'NA', 'After 9 January 2021']:\n",
    "        return pd.to_datetime('2021-01-09')\n",
    "    \n",
    "    try:\n",
    "        return pd.to_datetime(end_date_str,  dayfirst=True, infer_datetime_format=True)\n",
    "    except ValueError:\n",
    "        day = int(re.search('[0-9]+', end_date_str)[0])\n",
    "        for month_str, (m, y) in lookup_months.items():\n",
    "            if month_str in end_date_str:\n",
    "                dt = pd.Timestamp(day=day, month=m, year=y)\n",
    "                print(f'Regex Succeeded: Converted {end_date_str} to {dt}')\n",
    "                return dt\n",
    "        \n",
    "        print(f'Could not convert end date {end_date_str}')\n",
    "                    \n",
    "        \n",
    "        \n",
    "def process_value(value):\n",
    "    if str(value).strip() in ['no', 'No', 'nan', 'NaN']:\n",
    "        return 0\n",
    "    else:\n",
    "        return int(value)\n",
    "        \n",
    "def process_cm_dict(row, cm_dict):\n",
    "    sd = str(row[cm_dict['start_date_col']])\n",
    "    ed = str(row[cm_dict['end_date_col']])\n",
    "\n",
    "    sd_dt = process_start_date_str(sd)\n",
    "    ed_dt = process_end_date_str(ed)\n",
    "\n",
    "    if sd_dt is None:\n",
    "        return (None, None, None)\n",
    "    else:\n",
    "        # sd is not None\n",
    "        value = 1 if cm_dict['type'] == 'binary' else process_value(row[cm_dict['value_col']])\n",
    "        return (sd_dt, ed_dt, value)\n",
    "        \n",
    "def datetime_to_index(dt, Ds):\n",
    "    ind = None\n",
    "    \n",
    "    if dt < pd.to_datetime('2020-08-01'):\n",
    "        ind = 0\n",
    "    else:\n",
    "        try:\n",
    "            ind = list(Ds).index(dt)\n",
    "        except:\n",
    "            error_str = f'Date {dt} was not in my list'\n",
    "            new_dt = dt - pd.DateOffset(years=1)\n",
    "            if new_dt in Ds:\n",
    "                ind = list(Ds).index(new_dt)\n",
    "                error_str = f'{error_str} -- Used month and date'\n",
    "            else:\n",
    "                error_str = f'{error_str} -- failed'\n",
    "            print(error_str)\n",
    "    return ind\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_active_cms_mat(df, Rs, npi_information_dict, Ds):\n",
    "    CMs = list(npi_information_dict.keys())\n",
    "\n",
    "    nRs = len(Rs)\n",
    "    nDs = len(Ds)\n",
    "    nCMs = len(CMs)\n",
    "    \n",
    "    active_cms = np.zeros((nRs, nCMs, nDs))\n",
    "    \n",
    "    for r_i, r in enumerate(Rs):\n",
    "        sub_df = df.loc[r]\n",
    "        for _, row in sub_df.iterrows():\n",
    "            for cm_i, (cm_name, cm_dict) in enumerate(npi_information_dict.items()):\n",
    "                sd_dt, ed_dt, value = process_cm_dict(row, cm_dict)\n",
    "\n",
    "                if sd_dt is not None and ed_dt is not None:\n",
    "                    start_ind = datetime_to_index(sd_dt, Ds)           \n",
    "                    end_ind = datetime_to_index(ed_dt, Ds)\n",
    "                    # the NPI should be active on the end date. that's why we need the \"+1\"\n",
    "                    active_cms[r_i, cm_i, start_ind:end_ind+1] = value\n",
    "    return active_cms\n",
    "\n",
    "\n",
    "def load_new_cases_deaths_from_timeseries_df(Rs, timeseries_df, Ds):\n",
    "    new_cases = np.zeros((len(Rs), len(Ds)))\n",
    "    new_deaths = np.zeros((len(Rs), len(Ds)))\n",
    "    \n",
    "    for r_i, r in enumerate(Rs):\n",
    "        new_cases[r_i, :] = timeseries_df.loc[r].loc[Ds]['new_cases']\n",
    "        new_deaths[r_i, :] = timeseries_df.loc[r].loc[Ds]['new_deaths']\n",
    "        \n",
    "    return new_cases, new_deaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all data\n",
    "\n",
    "## UK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently reads directly from the CSV\n",
    "uk_df = pd.read_csv('../../data/npi_data/england.csv', skiprows=2).dropna(axis='index', how='all')#.set_index('Code')\n",
    "droplist = [c for c in uk_df.columns if 'Sources' in c or 'Quotes' in c or 'Description' in c or 'What is the reason' in c or 'How many' in c or 'Unnamed' in c]\n",
    "droplist.extend(['Person who entered this row', 'At any point in time, did the local area ever implement NPIs of interest in only some part of the local area. If yes, describe the situation a bit more.',\n",
    "       'Would it be very easy to collect further data on neighboring local areas? If yes, for which local areas?',\n",
    "       'How long did you need to collect this data?'])\n",
    "\n",
    "uk_df = uk_df.drop(droplist, axis=1)\n",
    "uk_df = uk_df.rename(columns=lambda x: x.strip())\n",
    "uk_df['Local area'] = uk_df['Local area'].apply(lambda x: str(x).strip())\n",
    "\n",
    "uk_df = uk_df.set_index('Local area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_Rs = ['Lincolnshire', 'Greater Manchester South West',\n",
    "       'Redbridge and Waltham Forest', 'Enfield', 'Buckinghamshire CC',\n",
    "       'Portsmouth', 'Southampton', 'Brighton and Hove', 'Coventry',\n",
    "       'Walsall', 'North Yorkshire CC', 'Essex Haven Gateway',\n",
    "       'Southend-on-Sea', 'Gloucestershire', 'East Derbyshire']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '08-01-2020'\n",
    "end_date = '01-09-2021'\n",
    "Ds = pd.date_range(start=start_date, end=end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date 2021-11-05 00:00:00 was not in my list -- Used month and date\n",
      "Regex Succeeded: Converted January 3 to 2021-01-03 00:00:00\n",
      "Date 2021-06-15 00:00:00 was not in my list -- failed\n",
      "Date 2021-11-05 00:00:00 was not in my list -- Used month and date\n",
      "Date 2021-09-08 00:00:00 was not in my list -- Used month and date\n",
      "Regex Succeeded: Converted August 31 to 2020-08-31 00:00:00\n",
      "Regex Succeeded: Converted August 31 to 2020-08-31 00:00:00\n",
      "Date 2021-11-05 00:00:00 was not in my list -- Used month and date\n",
      "Date 2021-11-05 00:00:00 was not in my list -- Used month and date\n",
      "Date 2021-11-05 00:00:00 was not in my list -- Used month and date\n",
      "Regex Succeeded: Converted 20 December to 2020-12-20 00:00:00\n",
      "Regex Succeeded: Converted 20 December to 2020-12-20 00:00:00\n",
      "Regex Succeeded: Converted 20 December to 2020-12-20 00:00:00\n",
      "Date 2021-11-05 00:00:00 was not in my list -- Used month and date\n",
      "Regex Succeeded: Converted 20 December to 2020-12-20 00:00:00\n",
      "Regex Succeeded: Converted 19 December to 2020-12-19 00:00:00\n",
      "Regex Succeeded: Converted 19 December to 2020-12-19 00:00:00\n",
      "Regex Succeeded: Converted 20 December to 2020-12-20 00:00:00\n",
      "Regex Succeeded: Converted 20 December to 2020-12-20 00:00:00\n",
      "Date 2021-11-05 00:00:00 was not in my list -- Used month and date\n",
      "Regex Succeeded: Converted January 3 to 2021-01-03 00:00:00\n",
      "Regex Succeeded: Converted 25 December to 2020-12-25 00:00:00\n",
      "Regex Succeeded: Converted 25 December to 2020-12-25 00:00:00\n",
      "Regex Succeeded: Converted 25 December to 2020-12-25 00:00:00\n",
      "Regex Succeeded: Converted 25 December to 2020-12-25 00:00:00\n",
      "Date 2021-11-05 00:00:00 was not in my list -- Used month and date\n",
      "Regex Succeeded: Converted January 3 to 2021-01-03 00:00:00\n",
      "Regex Succeeded: Converted 25 December to 2020-12-25 00:00:00\n",
      "Regex Succeeded: Converted 25 December to 2020-12-25 00:00:00\n",
      "Regex Succeeded: Converted 25 December to 2020-12-25 00:00:00\n",
      "Regex Succeeded: Converted 25 December to 2020-12-25 00:00:00\n",
      "Date 2021-11-05 00:00:00 was not in my list -- Used month and date\n",
      "Regex Succeeded: Converted January 3 to 2021-01-03 00:00:00\n",
      "Date 2021-11-05 00:00:00 was not in my list -- Used month and date\n",
      "Regex Succeeded: Converted January 3 to 2021-01-03 00:00:00\n",
      "Date 2021-11-05 00:00:00 was not in my list -- Used month and date\n",
      "Date 2021-11-05 00:00:00 was not in my list -- Used month and date\n",
      "Regex Succeeded: Converted September 1 to 2020-09-01 00:00:00\n",
      "Regex Succeeded: Converted September 1 to 2020-09-01 00:00:00\n",
      "Date 2021-10-17 00:00:00 was not in my list -- Used month and date\n",
      "Date 2021-10-17 00:00:00 was not in my list -- Used month and date\n",
      "Date 2021-12-15 00:00:00 was not in my list -- Used month and date\n",
      "Date 2021-12-15 00:00:00 was not in my list -- Used month and date\n",
      "Regex Succeeded: Converted December 20 to 2020-12-20 00:00:00\n",
      "Regex Succeeded: Converted December 20 to 2020-12-20 00:00:00\n",
      "Date 2021-11-05 00:00:00 was not in my list -- Used month and date\n",
      "Regex Succeeded: Converted September 1 to 2020-09-01 00:00:00\n",
      "Regex Succeeded: Converted September 1 to 2020-09-01 00:00:00\n",
      "Date 2021-12-15 00:00:00 was not in my list -- Used month and date\n",
      "Date 2021-12-15 00:00:00 was not in my list -- Used month and date\n",
      "Regex Succeeded: Converted December 20 to 2020-12-20 00:00:00\n",
      "Regex Succeeded: Converted December 20 to 2020-12-20 00:00:00\n",
      "Date 2021-11-05 00:00:00 was not in my list -- Used month and date\n",
      "Date 2021-08-31 00:00:00 was not in my list -- Used month and date\n",
      "Date 2021-08-31 00:00:00 was not in my list -- Used month and date\n",
      "Date 2021-11-05 00:00:00 was not in my list -- Used month and date\n",
      "Regex Succeeded: Converted January 3 to 2021-01-03 00:00:00\n"
     ]
    }
   ],
   "source": [
    "uk_active_cms = create_active_cms_mat(uk_df, uk_Rs, npi_information_dict, Ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/raw_data_w_sources/uk_ltla_info.json') as json_file:\n",
    "    uk_ltla_info_dict = json.load(json_file)\n",
    "\n",
    "uk_ltla_info_df = pd.DataFrame([d['attributes'] for d in uk_ltla_info_dict['features']])\n",
    "uk_ltla_info_df = uk_ltla_info_df.rename({'LAU117NM': 'area', 'NUTS318NM': 'NUTS3', 'NUTS118NM': 'region'} ,axis=1)\n",
    "uk_ltla_info_df = uk_ltla_info_df.set_index('area')\n",
    "\n",
    "uk_df = pd.read_csv('../../data/raw_data_w_sources/uk_case_deaths.csv', infer_datetime_format=True)\n",
    "uk_df = uk_df.drop(['areaCode', 'newCasesByPublishDate', 'newDeaths28DaysByPublishDate'], axis=1)\n",
    "uk_df['areaType'] = 'UK'\n",
    "uk_df = uk_df.rename({'areaType': 'country', 'areaName':'area', 'newCasesBySpecimenDate': 'new_cases', 'newDeaths28DaysByDeathDate': 'new_deaths'}, axis=1)\n",
    "uk_df = uk_df.set_index(['area', 'date'])\n",
    "\n",
    "def NUTS3_lookup(ltla):\n",
    "    try:\n",
    "        nuts3 = uk_ltla_info_df.loc[ltla]['NUTS3']\n",
    "    except KeyError:\n",
    "#         print(f'{ltla} missing in my lookup table')\n",
    "        nuts3 = 'unknown'\n",
    "    return nuts3\n",
    "\n",
    "nuts3_uk_df = uk_df.reset_index()\n",
    "nuts3_uk_df['NUTS3'] = nuts3_uk_df['area'].map(NUTS3_lookup)\n",
    "days = nuts3_uk_df['date'].unique()\n",
    "nuts3_regions = nuts3_uk_df['NUTS3'].unique()\n",
    "nuts3_df_list = []\n",
    "nuts3_uk_df_merged = None\n",
    "\n",
    "for nuts3_region in nuts3_regions:\n",
    "    if nuts3_region == 'unknown':\n",
    "        continue\n",
    "    \n",
    "    filtered_df = nuts3_uk_df.loc[nuts3_uk_df['NUTS3'] == nuts3_region]\n",
    "    \n",
    "    case_death_series = filtered_df.groupby('date').sum()\n",
    "    case_death_series['area'] = nuts3_region\n",
    "    \n",
    "    if nuts3_uk_df_merged is None:\n",
    "        nuts3_uk_df_merged = copy.deepcopy(case_death_series)\n",
    "    else:\n",
    "        nuts3_uk_df_merged = nuts3_uk_df_merged.append(case_death_series)\n",
    "    \n",
    "nuts3_uk_df_merged = nuts3_uk_df_merged.reset_index()\n",
    "nuts3_uk_df_merged['date'] = pd.to_datetime(nuts3_uk_df_merged['date'])\n",
    "nuts3_uk_df_merged = nuts3_uk_df_merged.set_index(['area', 'date'])\n",
    "nuts3_uk_df_merged = nuts3_uk_df_merged.sort_index(level=[1],ascending=[True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_cases, uk_deaths = load_new_cases_deaths_from_timeseries_df(uk_Rs, nuts3_uk_df_merged, Ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Austria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently reads directly from the CSV\n",
    "at_df = pd.read_csv('../../data/npi_data/austria.csv', skiprows=2).dropna(axis='index', how='all')#.set_index('Code')\n",
    "droplist = [c for c in at_df.columns if 'Sources' in c or 'Quotes' in c or 'Description' in c or 'What is the reason' in c or 'How many' in c or 'Unnamed' in c]\n",
    "droplist.extend(['Person who entered this row', 'At any point in time, did the local area ever implement NPIs of interest in only some part of the local area. If yes, describe the situation a bit more.',\n",
    "       'Would it be very easy to collect further data on neighboring local areas? If yes, for which local areas?',\n",
    "       'How long did you need to collect this data?', 'Local area'])\n",
    "\n",
    "at_df = at_df.drop(droplist, axis=1)\n",
    "at_df = at_df.rename(columns=lambda x: x.strip())\n",
    "\n",
    "at_df = at_df.set_index('Region')\n",
    "\n",
    "at_Rs = ['Wien', 'Burgenland', 'Steiermark', 'Oberösterreich',\n",
    "       'Nieder­österreich', 'Voralberg ', 'Tirol', 'Karnten/Carinthia',\n",
    "       'Salzburg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regex Succeeded: Converted 6 Jan to 2021-01-06 00:00:00\n",
      "Regex Succeeded: Converted 27 September to 2020-09-27 00:00:00\n",
      "Regex Succeeded: Converted 27 September to 2020-09-27 00:00:00\n",
      "Could not convert start date 26 Decemeber \n",
      "Date 2021-11-17 00:00:00 was not in my list -- Used month and date\n",
      "Date 2021-12-06 00:00:00 was not in my list -- Used month and date\n",
      "Date 2021-11-17 00:00:00 was not in my list -- Used month and date\n",
      "Date 2021-12-06 00:00:00 was not in my list -- Used month and date\n"
     ]
    }
   ],
   "source": [
    "at_active_cms = create_active_cms_mat(at_df, at_Rs, npi_information_dict, Ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "austria_ltla_lookup = pd.read_csv('../../data/raw_data_w_sources/at_lau_lookup.csv')\n",
    "austria_ltla_lookup = austria_ltla_lookup.set_index('GKZ')\n",
    "\n",
    "def at_ltla_lookup(ltla):\n",
    "    if ltla in austria_ltla_lookup.index:\n",
    "        return austria_ltla_lookup.loc[ltla]['State Code (middle column of HASC)']\n",
    "    return 'Vienna'\n",
    "\n",
    "austria_df = pd.read_csv('../../data/raw_data_w_sources/at_case_deaths.csv', error_bad_lines=False, delimiter=';', skiprows=1)\n",
    "austria_df = austria_df.drop([' number of cases total',\n",
    "       ' number of cases of 7 days', ' seven days of incidence cases',' number of total totals',\n",
    "       ' number of held daily', ' number of healing total'], axis=1)\n",
    "austria_df[' GKZ'] = austria_df[' GKZ'].map(at_ltla_lookup)\n",
    "austria_df = austria_df.rename({'Time': 'date', ' district': 'area', ' GKZ': 'region', ' number of inhabitants': 'population', ' number of cases': 'new_cases', ' number of dead daily': 'new_deaths'}, axis=1)\n",
    "austria_df = austria_df.drop('population', axis=1)\n",
    "austria_df['date'] = pd.to_datetime(austria_df['date'], format='%d.%m.%Y %M:%H:%S')\n",
    "\n",
    "austria_timeseries_df = austria_df.set_index(['area', 'date'])\n",
    "\n",
    "austria_nuts2_regions = austria_timeseries_df['region'].unique()\n",
    "\n",
    "austria_nuts2_df_list = []\n",
    "\n",
    "austria_nuts2_df_merged = None\n",
    "\n",
    "for nuts2_region in austria_nuts2_regions:    \n",
    "    filtered_df = austria_timeseries_df.loc[austria_timeseries_df['region'] == nuts2_region]\n",
    "    \n",
    "    case_death_series = filtered_df.groupby('date').sum()\n",
    "    case_death_series['area'] = nuts2_region\n",
    "    \n",
    "    if austria_nuts2_df_merged is None:\n",
    "        austria_nuts2_df_merged = copy.deepcopy(case_death_series)\n",
    "    else:\n",
    "        austria_nuts2_df_merged = austria_nuts2_df_merged.append(case_death_series)\n",
    "    \n",
    "austria_nuts2_df_merged = austria_nuts2_df_merged.reset_index()\n",
    "austria_nuts2_df_merged = austria_nuts2_df_merged.set_index(['area', 'date'])\n",
    "austria_nuts2_df_merged = austria_nuts2_df_merged.sort_index(level=[1],ascending=[True])\n",
    "\n",
    "austria_nuts2_timeseries_df = austria_nuts2_df_merged\n",
    "\n",
    "timeseries_df_map_dict = {\n",
    "    'BU': 'Burgenland',\n",
    "    'Vienna': 'Wien',\n",
    "    'ST': 'Steiermark',\n",
    "    'OO': 'Oberösterreich',\n",
    "    'TR': 'Tirol',\n",
    "    'VO': 'Voralberg ',\n",
    "    'KA': 'Karnten/Carinthia',\n",
    "    'OO': 'Oberösterreich',\n",
    "    'NO': 'Nieder\\xadösterreich',\n",
    "    'SZ': 'Salzburg',\n",
    "}\n",
    "\n",
    "austria_nuts2_timeseries_df.index = austria_nuts2_timeseries_df.index.map(lambda x: (timeseries_df_map_dict[x[0]], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "at_cases, at_deaths = load_new_cases_deaths_from_timeseries_df(at_Rs, austria_nuts2_timeseries_df, Ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Germany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently reads directly from the CSV\n",
    "de_df = pd.read_csv('../../data/npi_data/germany.csv', skiprows=2).dropna(axis='index', how='all')#.set_index('Code')\n",
    "droplist = [c for c in de_df.columns if 'Sources' in c or 'Quotes' in c or 'Description' in c or 'What is the reason' in c or 'How many' in c or 'Unnamed' in c]\n",
    "droplist.extend(['Person who entered this row', 'At any point in time, did the local area ever implement NPIs of interest in only some part of the local area. If yes, describe the situation a bit more.',\n",
    "       'Would it be very easy to collect further data on neighboring local areas? If yes, for which local areas?',\n",
    "       'How long did you need to collect this data?'])\n",
    "\n",
    "de_df = de_df.drop(droplist, axis=1)\n",
    "de_df = de_df.rename(columns=lambda x: x.strip())\n",
    "de_df['Local area'] = de_df['Local area'].apply(lambda x: str(x).strip())\n",
    "\n",
    "de_Rs = ['Nürnberg', 'LK Aschaffenburg', 'Fürth', 'Landsberg am Lech',\n",
    "       'LK Donau-Ries', 'Minden-Lübbecke', 'Mönchengladbach', 'Münster',\n",
    "       'Rhein-Kreis Neuss', 'LK Ennepe-Ruhr-Kreis', 'LK Rems-Murr-Kreis',\n",
    "       'LK Breisgau-Hochschwarzwald', 'LK Enzkreis', 'LK Hildesheim',\n",
    "       'LK Gifhorn']\n",
    "de_df = de_df.set_index('Local area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_active_cms = create_active_cms_mat(de_df, de_Rs, npi_information_dict, Ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ags dict contains information about the local areas of germany\n",
    "with open('../../data/raw_data_w_sources/de_ags.json') as json_file:\n",
    "    ags_info_dict = json.load(json_file)\n",
    "    \n",
    "cases_df = pd.read_csv('../../data/raw_data_w_sources/de_cases-rki-by-ags.csv')\n",
    "cases_df = cases_df.drop('sum_cases', axis=1)\n",
    "cases_df = cases_df.rename({'time_iso8601': 'date'}, axis=1)\n",
    "cases_df['date'] = pd.to_datetime(cases_df['date'])\n",
    "cases_df['date'] = pd.to_datetime(cases_df['date'].dt.date)\n",
    "cases_df = cases_df.set_index('date')\n",
    "cases_df = cases_df.diff()\n",
    "\n",
    "deaths_df = pd.read_csv('../../data/raw_data_w_sources/de_deaths-rki-by-ags.csv')\n",
    "deaths = deaths_df.drop('sum_deaths', axis=1)\n",
    "deaths_df = deaths_df.rename({'time_iso8601': 'date'}, axis=1)\n",
    "deaths_df['date'] = pd.to_datetime(deaths_df['date'])\n",
    "deaths_df['date'] = pd.to_datetime(deaths_df['date'].dt.date)\n",
    "deaths_df = deaths_df.set_index('date')\n",
    "deaths_df = deaths_df.diff()\n",
    "\n",
    "ags_time_series_list = []\n",
    "\n",
    "Ds = pd.date_range('2020-03-02', '2021-01-09')\n",
    "for ags in ags_info_dict.keys():\n",
    "    if ags == '3152':\n",
    "        continue\n",
    "        \n",
    "    for d in Ds:\n",
    "        ags_dict = {\n",
    "            'area': ags_info_dict[ags]['name'],\n",
    "            'date': d\n",
    "        }\n",
    "        ags_dict['new_cases'] = cases_df[ags][d]\n",
    "        ags_dict['new_deaths'] = deaths_df[ags][d]\n",
    "        \n",
    "        ags_time_series_list.append(ags_dict)\n",
    "\n",
    "germany_timeseries_df = pd.DataFrame(ags_time_series_list)\n",
    "germany_timeseries_df = germany_timeseries_df.set_index(['area', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_Rs_conv = [\n",
    " 'SK Nürnberg',\n",
    " 'LK Aschaffenburg',\n",
    " 'LK Fürth',\n",
    " 'LK Landsberg a.Lech',\n",
    " 'LK Donau-Ries',\n",
    " 'LK Minden-Lübbecke',\n",
    " 'SK Mönchengladbach',\n",
    " 'SK Münster',\n",
    " 'LK Rhein-Kreis Neuss',\n",
    " 'LK Ennepe-Ruhr-Kreis',\n",
    " 'LK Rems-Murr-Kreis',\n",
    " 'LK Breisgau-Hochschwarzwald',\n",
    " 'LK Enzkreis',\n",
    " 'LK Hildesheim',\n",
    " 'LK Gifhorn' \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cases and deaths from SK Nürnberg, NPIs from Nürnberg\n",
      "Cases and deaths from LK Aschaffenburg, NPIs from LK Aschaffenburg\n",
      "Cases and deaths from LK Fürth, NPIs from Fürth\n",
      "Cases and deaths from LK Landsberg a.Lech, NPIs from Landsberg am Lech\n",
      "Cases and deaths from LK Donau-Ries, NPIs from LK Donau-Ries\n",
      "Cases and deaths from LK Minden-Lübbecke, NPIs from Minden-Lübbecke\n",
      "Cases and deaths from SK Mönchengladbach, NPIs from Mönchengladbach\n",
      "Cases and deaths from SK Münster, NPIs from Münster\n",
      "Cases and deaths from LK Rhein-Kreis Neuss, NPIs from Rhein-Kreis Neuss\n",
      "Cases and deaths from LK Ennepe-Ruhr-Kreis, NPIs from LK Ennepe-Ruhr-Kreis\n",
      "Cases and deaths from LK Rems-Murr-Kreis, NPIs from LK Rems-Murr-Kreis\n",
      "Cases and deaths from LK Breisgau-Hochschwarzwald, NPIs from LK Breisgau-Hochschwarzwald\n",
      "Cases and deaths from LK Enzkreis, NPIs from LK Enzkreis\n",
      "Cases and deaths from LK Hildesheim, NPIs from LK Hildesheim\n",
      "Cases and deaths from LK Gifhorn, NPIs from LK Gifhorn\n"
     ]
    }
   ],
   "source": [
    "for cs_name, npi_name in zip(de_Rs_conv, de_Rs):\n",
    "    print(f'Cases and deaths from {cs_name}, NPIs from {npi_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '08-01-2020'\n",
    "end_date = '01-09-2021'\n",
    "Ds = pd.date_range(start=start_date, end=end_date)\n",
    "\n",
    "de_cases, de_deaths = load_new_cases_deaths_from_timeseries_df(de_Rs_conv, germany_timeseries_df, Ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Italy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently reads directly from the CSV\n",
    "it_df = pd.read_csv('../../data/npi_data/italy.csv', skiprows=2).dropna(axis='index', how='all')#.set_index('Code')\n",
    "droplist = [c for c in it_df.columns if 'Sources' in c or 'Quotes' in c or 'Description' in c or 'What is the reason' in c or 'How many' in c or 'Unnamed' in c]\n",
    "droplist.extend(['Person who entered this row', 'At any point in time, did the local area ever implement NPIs of interest in only some part of the local area. If yes, describe the situation a bit more.',\n",
    "       'Would it be very easy to collect further data on neighboring local areas? If yes, for which local areas?',\n",
    "       'How long did you need to collect this data?', 'Local area'])\n",
    "\n",
    "it_df = it_df.drop(droplist, axis=1)\n",
    "it_df = it_df.rename(columns=lambda x: x.strip())\n",
    "\n",
    "it_df = it_df.set_index('Region')\n",
    "\n",
    "it_Rs = ['Abruzzo', 'Aosta Valley', 'Apulia (AKA Puglia)','Basilicata','Calabria','Campania','Emilia-Romagna',\n",
    "        'Friuli-Venezia Giulia','Lazio','Liguria ','Lombardy','Marche','Molise','Piedmont','Sardinia','Sicily',\n",
    "         'Trentino (aka Trento)', 'South Tyrol (aka Bolzano aka Alto-Adige)', 'Tuscania ','Umbria','Veneto']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regex Succeeded: Converted 21 Septemeber 2020 to 2020-09-21 00:00:00\n",
      "Date 2021-12-11 00:00:00 was not in my list -- Used month and date\n",
      "Regex Succeeded: Converted 6 November to 2020-11-06 00:00:00\n",
      "Regex Succeeded: Converted 14 November to 2020-11-14 00:00:00\n",
      "Regex Succeeded: Converted 14 November to 2020-11-14 00:00:00\n",
      "Regex Succeeded: Converted 14 November to 2020-11-14 00:00:00\n",
      "Regex Succeeded: Converted 14 November to 2020-11-14 00:00:00\n",
      "Regex Succeeded: Converted 6 November to 2020-11-06 00:00:00\n"
     ]
    }
   ],
   "source": [
    "it_active_cms = create_active_cms_mat(it_df, it_Rs, npi_information_dict, Ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "italy_df = pd.read_csv('../../data/raw_data_w_sources/it_cases_deaths.csv', delimiter=',')\n",
    "italy_df['date'] = pd.to_datetime(italy_df['date'])\n",
    "italy_df['date'] = italy_df['date'].dt.date\n",
    "italy_df = italy_df.set_index(['area', 'date'])\n",
    "italy_df['new_deaths'] = italy_df.groupby('area').diff()['total_deaths']\n",
    "italy_df = italy_df.drop('total_deaths', axis=1)\n",
    "\n",
    "italy_timeseries_df = italy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_Rs_conv = ['Abruzzo',\n",
    " \"Valle d'Aosta\",\n",
    " 'Puglia',\n",
    " 'Basilicata',\n",
    " 'Calabria',\n",
    " 'Campania',\n",
    " 'Emilia-Romagna',\n",
    " 'Friuli Venezia Giulia',\n",
    " 'Lazio',\n",
    " 'Liguria',\n",
    " 'Lombardia',\n",
    " 'Marche',\n",
    " 'Molise',\n",
    " 'Piemonte',\n",
    " 'Sardegna',\n",
    " 'Sicilia',\n",
    " 'P.A. Trento',\n",
    " 'P.A. Bolzano', # or P.A. Trento\n",
    " 'Toscana',\n",
    " 'Umbria',\n",
    " 'Veneto']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cases and deaths from Abruzzo, NPIs from Abruzzo\n",
      "Cases and deaths from Valle d'Aosta, NPIs from Aosta Valley\n",
      "Cases and deaths from Puglia, NPIs from Apulia (AKA Puglia)\n",
      "Cases and deaths from Basilicata, NPIs from Basilicata\n",
      "Cases and deaths from Calabria, NPIs from Calabria\n",
      "Cases and deaths from Campania, NPIs from Campania\n",
      "Cases and deaths from Emilia-Romagna, NPIs from Emilia-Romagna\n",
      "Cases and deaths from Friuli Venezia Giulia, NPIs from Friuli-Venezia Giulia\n",
      "Cases and deaths from Lazio, NPIs from Lazio\n",
      "Cases and deaths from Liguria, NPIs from Liguria \n",
      "Cases and deaths from Lombardia, NPIs from Lombardy\n",
      "Cases and deaths from Marche, NPIs from Marche\n",
      "Cases and deaths from Molise, NPIs from Molise\n",
      "Cases and deaths from Piemonte, NPIs from Piedmont\n",
      "Cases and deaths from Sardegna, NPIs from Sardinia\n",
      "Cases and deaths from Sicilia, NPIs from Sicily\n",
      "Cases and deaths from P.A. Trento, NPIs from Trentino (aka Trento)\n",
      "Cases and deaths from P.A. Bolzano, NPIs from South Tyrol (aka Bolzano aka Alto-Adige)\n",
      "Cases and deaths from Toscana, NPIs from Tuscania \n",
      "Cases and deaths from Umbria, NPIs from Umbria\n",
      "Cases and deaths from Veneto, NPIs from Veneto\n"
     ]
    }
   ],
   "source": [
    "for cs_name, npi_name in zip(it_Rs_conv, it_Rs):\n",
    "    print(f'Cases and deaths from {cs_name}, NPIs from {npi_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_cases, it_deaths = load_new_cases_deaths_from_timeseries_df(it_Rs_conv, italy_timeseries_df, Ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create merged CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charlie/.cache/pypoetry/virtualenvs/covid19npisecondwave-rNDyq_Fa-py3.8/lib/python3.8/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    }
   ],
   "source": [
    "CMs = list(npi_information_dict.keys())\n",
    "countries = ['England', 'Austria', 'Germany', 'Italy']\n",
    "new_cases = [uk_cases, at_cases, de_cases, it_cases]\n",
    "new_deaths = [uk_deaths, at_deaths, de_deaths, it_deaths]\n",
    "active_cms = [uk_active_cms, at_active_cms, de_active_cms, it_active_cms]\n",
    "np.save('../../data/active_cms', active_cms)\n",
    "Rs = [uk_Rs, at_Rs, de_Rs, it_Rs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Lincolnshire', 'Greater Manchester South West', 'Redbridge and Waltham Forest', 'Enfield', 'Buckinghamshire CC', 'Portsmouth', 'Southampton', 'Brighton and Hove', 'Coventry', 'Walsall', 'North Yorkshire CC', 'Essex Haven Gateway', 'Southend-on-Sea', 'Gloucestershire', 'East Derbyshire'], ['Wien', 'Burgenland', 'Steiermark', 'Oberösterreich', 'Nieder\\xadösterreich', 'Voralberg ', 'Tirol', 'Karnten/Carinthia', 'Salzburg'], ['Nürnberg', 'LK Aschaffenburg', 'Fürth', 'Landsberg am Lech', 'LK Donau-Ries', 'Minden-Lübbecke', 'Mönchengladbach', 'Münster', 'Rhein-Kreis Neuss', 'LK Ennepe-Ruhr-Kreis', 'LK Rems-Murr-Kreis', 'LK Breisgau-Hochschwarzwald', 'LK Enzkreis', 'LK Hildesheim', 'LK Gifhorn'], ['Abruzzo', 'Aosta Valley', 'Apulia (AKA Puglia)', 'Basilicata', 'Calabria', 'Campania', 'Emilia-Romagna', 'Friuli-Venezia Giulia', 'Lazio', 'Liguria ', 'Lombardy', 'Marche', 'Molise', 'Piedmont', 'Sardinia', 'Sicily', 'Trentino (aka Trento)', 'South Tyrol (aka Bolzano aka Alto-Adige)', 'Tuscania ', 'Umbria', 'Veneto']]\n"
     ]
    }
   ],
   "source": [
    "print(Rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "England\n",
      "Austria\n",
      "Germany\n",
      "Italy\n"
     ]
    }
   ],
   "source": [
    "all_rows = []\n",
    " \n",
    "for c, ncs, nds, acms, rs in zip(countries, new_cases, new_deaths, active_cms, Rs):\n",
    "    print(c)\n",
    "    for r_i, r in enumerate(rs):\n",
    "        for d_i, d in enumerate(Ds):\n",
    "            row_dict = {\n",
    "                'Country': c,\n",
    "                'Area': r,\n",
    "                'Date': d,\n",
    "                'New Cases': ncs[r_i, d_i],\n",
    "                'New Deaths': nds[r_i, d_i]\n",
    "            }\n",
    "            \n",
    "            for cm_i, cm in enumerate(CMs):\n",
    "                row_dict[cm] = acms[r_i, cm_i, d_i]\n",
    "\n",
    "            all_rows.append(row_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.DataFrame(all_rows).set_index(['Area', 'Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('../../data/all_merged_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-d6daf54e",
   "language": "python",
   "display_name": "PyCharm (COVID19NPISecondWave)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [
     "\n"
    ],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}