{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Merging NPI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "import seaborn as sns\n",
    "import json\n",
    "import copy\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this dict contains information for loading the info\n",
    "npi_information_dict = {\n",
    "    'Public Outdoor Gathering Person Limit': {\n",
    "        'type': 'value',\n",
    "        'start_date_col': 'Start date (Public Outdoors)',\n",
    "        'end_date_col': 'End date (Public Outdoors)',\n",
    "        'value_col': 'Limit on number of people (only X people or fewer) (Public Outdoors)'\n",
    "    },\n",
    "    'Public Indoor Gathering Person Limit': {\n",
    "        'type': 'value',\n",
    "        'start_date_col': 'Start date (Public Indoors)',\n",
    "        'end_date_col': 'End date (Public Indoors)',\n",
    "        'value_col': 'Limit on number of people (only X people or fewer) (Public Indoors)'\n",
    "    },\n",
    "    'Private Outdoor Gathering Person Limit': {\n",
    "        'type': 'value',\n",
    "        'start_date_col': 'Start date (Private Outdoors)',\n",
    "        'end_date_col': 'End date (Private Outdoors)',\n",
    "        'value_col': 'Limit on number of people (only X people or fewer) (Private Outdoors)'\n",
    "    },\n",
    "    'Private Indoor Gathering Person Limit': {\n",
    "        'type': 'value',\n",
    "        'start_date_col': 'Start date (Private Indoors)',\n",
    "        'end_date_col': 'End date (Private Indoors)',\n",
    "        'value_col': 'Limit on number of people (only X people or fewer) (Private Indoors)'\n",
    "    },\n",
    "    'Public Outdoor Household Limit': {\n",
    "        'type': 'value',\n",
    "        'start_date_col': 'Start date (Public Outdoors)',\n",
    "        'end_date_col': 'End date (Public Outdoors)',\n",
    "        'value_col': 'Limit on number of households (Public Outdoors)'\n",
    "    },\n",
    "    'Public Indoor Household Limit': {\n",
    "        'type': 'value',\n",
    "        'start_date_col': 'Start date (Public Indoors)',\n",
    "        'end_date_col': 'End date (Public Indoors)',\n",
    "        'value_col': 'Limit on number of households (Public Indoors)'\n",
    "    },\n",
    "    'Private Outdoor Household Limit': {\n",
    "        'type': 'value',\n",
    "        'start_date_col': 'Start date (Private Outdoors)',\n",
    "        'end_date_col': 'End date (Private Outdoors)',\n",
    "        'value_col': 'Limit on number of households (Private Outdoors)'\n",
    "    },\n",
    "    'Private Indoor Household Limit': {\n",
    "        'type': 'value',\n",
    "        'start_date_col': 'Start date (Private Indoors)',\n",
    "        'end_date_col': 'End date (Private Indoors)',\n",
    "        'value_col': 'Limit on number of households (Private Indoors)'\n",
    "    },\n",
    "    'Mandatory Mask Wearing': {\n",
    "        'type': 'value',\n",
    "        'start_date_col': 'Start date (Mask Wearing)',\n",
    "        'end_date_col': 'End date (Mask Wearing)',\n",
    "        'value_col': 'Level of NPI (0-4) (Mask Wearing)'\n",
    "    },\n",
    "    'Some Face-to-Face Businesses Closed': {\n",
    "        'type': 'binary',\n",
    "        'start_date_col': 'Start date (Some Face2Face)',\n",
    "        'end_date_col': 'End date (Some Face2Face)'\n",
    "    },\n",
    "    'Gastronomy Closed': {\n",
    "        'type': 'binary',\n",
    "        'start_date_col': 'Start date  (Gastronomy Closed)',\n",
    "        'end_date_col': 'End date (Gastronomy Closed)'\n",
    "    },\n",
    "    'Leisure Venues Closed': {\n",
    "        'type': 'binary',\n",
    "        'start_date_col': 'Start date (Leisure Venue)',\n",
    "        'end_date_col': 'End date (Leisure Venue)'\n",
    "    },\n",
    "    'Retail Closed': {\n",
    "        'type': 'binary',\n",
    "        'start_date_col': 'Start date (Retail)',\n",
    "        'end_date_col': 'End date (Retail)'\n",
    "    },\n",
    "    'All Face-to-Face Businesses Closed': {\n",
    "        'type': 'binary',\n",
    "        'start_date_col': 'Start date (All Face2Face)',\n",
    "        'end_date_col': 'End date (All Face2Face)'\n",
    "    },\n",
    "    'Curfew': {\n",
    "        'type': 'binary',\n",
    "        'start_date_col': 'Start date (Curfew)',\n",
    "        'end_date_col': 'End date (Curfew)'\n",
    "    },\n",
    "    'Primary Schools Closed': {\n",
    "        'type': 'binary',\n",
    "        'start_date_col': 'Start date (Primary Schools)',\n",
    "        'end_date_col': 'End date (Primary Schools)'\n",
    "    },\n",
    "    'Secondary Schools Closed': {\n",
    "        'type': 'binary',\n",
    "        'start_date_col': 'Start date (Secondary Schools)',\n",
    "        'end_date_col': 'End date (Secondary Schools)'\n",
    "    },\n",
    "    'Universities Away': {\n",
    "        'type': 'binary',\n",
    "        'start_date_col': 'Start date (Unis Away)',\n",
    "        'end_date_col': 'End date (Unis Away)'\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there is a string, this lookup converts the month string into the month int and year int\n",
    "# not the cleanest way of doing this....\n",
    "lookup_months = {\n",
    "    'January': (1, 2021),\n",
    "    'Jan': (1, 2021),\n",
    "    'September': (9, 2020),\n",
    "    'Septemeber': (9, 2020),\n",
    "    'December': (12, 2020),\n",
    "    'August': (8, 2020),\n",
    "    'November': (11, 2020),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "window_start_date = '2020-08-01'\n",
    "window_end_date = '2021-01-22'\n",
    "Ds = pd.date_range(start=window_start_date, end=window_end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_start_date_str(start_date_str):\n",
    "    if start_date_str.strip() in ['Before 1 August', 'before 1 August 2020', 'Before 1 August 2020', 'Before August 1', '1 August 2020', 'Before 1st of August', 'before 01/08/2020', 'before 1/08/2020',  \n",
    "                                  'before 1/8/2020', 'before 13/7/2020', 'Before 1st August', 'Before 01/08/2020']:\n",
    "        return pd.to_datetime('2020-08-01')\n",
    "    \n",
    "    elif start_date_str.strip() in ['no', 'No', 'nan', 'N/A', 'NA']:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        return pd.to_datetime(start_date_str, dayfirst=True, infer_datetime_format=True)\n",
    "    except ValueError:\n",
    "        day = int(re.search('[0-9]+', start_date_str)[0])\n",
    "        for month_str, (m, y) in lookup_months.items():\n",
    "            if month_str in start_date_str:\n",
    "                dt = pd.Timestamp(day=day, month=m, year=y)\n",
    "                print(f'Regex Succeeded: Converted {start_date_str} to {dt}')\n",
    "                return dt\n",
    "        print(f'Problem: Could not convert start date {start_date_str}')\n",
    "        \n",
    "def process_end_date_str(end_date_str):\n",
    "    if end_date_str.strip() in ['no', 'No', 'nan', 'N/A', 'NA', '-', 'After 9 January 2021', 'Na']:\n",
    "        return pd.to_datetime(window_end_date)\n",
    "\n",
    "    try:\n",
    "        return pd.to_datetime(end_date_str,  dayfirst=True, infer_datetime_format=True)\n",
    "    except ValueError:\n",
    "        print(end_date_str)\n",
    "        day = int(re.search('[0-9]+', end_date_str)[0])\n",
    "        for month_str, (m, y) in lookup_months.items():\n",
    "            if month_str in end_date_str:\n",
    "                dt = pd.Timestamp(day=day, month=m, year=y)\n",
    "                print(f'Regex Succeeded: Converted {end_date_str} to {dt}')\n",
    "                return dt\n",
    "        \n",
    "        print(f'Problem: Could not convert end date {end_date_str}')\n",
    "\n",
    "        \n",
    "def process_value(value):\n",
    "    if str(value).strip() in ['no', 'No', 'nan', 'NaN', 'Na']:\n",
    "        return 0\n",
    "    if ',' in str(value):\n",
    "        value = value.replace(',','')\n",
    "    return int(value)\n",
    "        \n",
    "def process_cm_dict(row, cm_dict):\n",
    "    sd = str(row[cm_dict['start_date_col']])\n",
    "    ed = str(row[cm_dict['end_date_col']])\n",
    "\n",
    "    sd_dt = process_start_date_str(sd)\n",
    "    ed_dt = process_end_date_str(ed)\n",
    "\n",
    "    if sd_dt is None:\n",
    "        return (None, None, None)\n",
    "    else:\n",
    "        # sd is not None\n",
    "        value = 1 if cm_dict['type'] == 'binary' else process_value(row[cm_dict['value_col']])\n",
    "        return (sd_dt, ed_dt, value)\n",
    "        \n",
    "def datetime_to_index(dt, Ds, r, cm_name, final):\n",
    "    ind = None\n",
    "    \n",
    "    if dt < pd.to_datetime('2020-08-01'):\n",
    "        if dt < pd.to_datetime('2020-06-01'):\n",
    "            print(f'Date {dt} is before 1st June. Region: {r}, CM: {cm_name}. Setting to Aug 1st')\n",
    "        ind = 0\n",
    "    else:\n",
    "        try:\n",
    "            ind = list(Ds).index(dt)\n",
    "        except:\n",
    "            if final == True:\n",
    "                print(f'Problem: Date {dt} is after {window_end_date}. Region: {r}, CM: {cm_name}. Setting to {window_end_date}')\n",
    "                return ind\n",
    "            else:\n",
    "                error_str = f'Date {dt} was not in my list'\n",
    "                new_dt = dt - pd.DateOffset(years=1)\n",
    "                if new_dt in Ds:\n",
    "                    ind = list(Ds).index(new_dt)\n",
    "                    error_str = f'{error_str} -- Used month and date'\n",
    "                else:\n",
    "                    error_str = f'Problem: {error_str} -- failed'\n",
    "                print(error_str)\n",
    "    return ind\n",
    "\n",
    "\n",
    "def create_active_cms_mat(df, Rs, npi_information_dict, Ds, final=True):\n",
    "    CMs = list(npi_information_dict.keys())\n",
    "\n",
    "    nRs = len(Rs)\n",
    "    nDs = len(Ds)\n",
    "    nCMs = len(CMs)\n",
    "\n",
    "    active_cms = np.zeros((nRs, nCMs, nDs))\n",
    "\n",
    "    for r_i, r in enumerate(Rs):\n",
    "        sub_df = df.loc[r]\n",
    "        for cm_i, (cm_name, cm_dict) in enumerate(npi_information_dict.items()):\n",
    "            old_end_ind = 0\n",
    "            for _, row in sub_df.iterrows():\n",
    "                sd_dt, ed_dt, value = process_cm_dict(row, cm_dict)\n",
    "                if sd_dt is not None and ed_dt is not None:\n",
    "                    start_ind = datetime_to_index(sd_dt, Ds, r, cm_name, final)\n",
    "                    end_ind = datetime_to_index(ed_dt, Ds, r, cm_name, final)\n",
    "                    if start_ind is None:\n",
    "                        print(f'Problem with start date: {sd_dt} for {cm_name} in {r}')\n",
    "                    else:\n",
    "                        if old_end_ind is not None:\n",
    "                            if start_ind < old_end_ind:\n",
    "                                print(f'Problem: start date is before the end date of the previous row on {sd_dt} for {cm_name} in {r}')\n",
    "                    if end_ind is None:\n",
    "                        print(f'Problem with end date: {ed_dt} for {cm_name} in {r}: setting to {window_end_date}')\n",
    "                        end_ind = len(Ds) - 1\n",
    "\n",
    "                    if start_ind is not None and end_ind is not None:\n",
    "                        if start_ind > end_ind:\n",
    "                            print(f'Problem with start-end date pair: [{Ds[start_ind]},{Ds[end_ind]}] for {cm_name} in {r}')\n",
    "                    # the NPI should be active on the end date. that's why we need the \"+1\"\n",
    "                    active_cms[r_i, cm_i, start_ind:end_ind+1] = value\n",
    "                    old_end_ind = end_ind\n",
    "    return active_cms\n",
    "\n",
    "def load_new_cases_deaths_from_timeseries_df(Rs, timeseries_df, Ds):\n",
    "    new_cases = np.zeros((len(Rs), len(Ds)))\n",
    "    new_deaths = np.zeros((len(Rs), len(Ds)))\n",
    "\n",
    "    for r_i, r in enumerate(Rs):\n",
    "        new_cases[r_i, :] = timeseries_df.loc[r].loc[Ds]['new_cases']\n",
    "        new_deaths[r_i, :] = timeseries_df.loc[r].loc[Ds]['new_deaths']\n",
    "\n",
    "    return new_cases, new_deaths\n",
    "\n",
    "def rename_university_col(df):\n",
    "    univ_col = [col for col in df.columns if 'How many universities' in col][0]\n",
    "    new_df = df.rename(columns={univ_col: 'Number of Universities'})\n",
    "    return new_df\n",
    "\n",
    "def no_universities(df, Rs):\n",
    "    new_df = df.copy()\n",
    "    for R in Rs:\n",
    "        # print(new_df.loc[R])\n",
    "        # print(new_df.loc[R, 'Number of Universities'].iloc[0][0])\n",
    "        if new_df.loc[R, 'Number of Universities'].iloc[0][0] == '0':\n",
    "            print('yass')\n",
    "            new_df.loc[R,'Start date (Unis Away)'] = 'Before 1 August'\n",
    "            new_df.loc[R,'End date (Unis Away)'] = 'no'\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load all data\n",
    "\n",
    "## UK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently reads directly from the CSV\n",
    "uk_df = pd.read_csv('../../data/npi_data/england.csv', skiprows=2).dropna(axis='index', how='all')#.set_index('Code')\n",
    "uk_df = rename_university_col(uk_df)\n",
    "droplist = [c for c in uk_df.columns if 'Sources' in c or 'Quotes' in c or 'Description' in c or 'What is the reason' in c or 'How many' in c or 'Unnamed' in c]\n",
    "\n",
    "uk_df = uk_df.drop(droplist, axis=1)\n",
    "uk_df = uk_df.rename(columns=lambda x: x.strip())\n",
    "uk_df['Local area'] = uk_df['Local area'].apply(lambda x: str(x).strip())\n",
    "\n",
    "uk_df = uk_df.set_index('Local area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_Rs = ['Lincolnshire', 'Greater Manchester South West',\n",
    "       'Redbridge and Waltham Forest', 'Enfield', 'Buckinghamshire CC',\n",
    "       'Portsmouth', 'Southampton', 'Brighton and Hove', 'Coventry',\n",
    "       'Walsall', 'North Yorkshire CC', 'Essex Haven Gateway',\n",
    "       'Southend-on-Sea', 'Gloucestershire', 'East Derbyshire']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_df = no_universities(uk_df, uk_Rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "uk_active_cms = create_active_cms_mat(uk_df, uk_Rs, npi_information_dict, Ds, final=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/raw_data_w_sources/uk_ltla_info.json') as json_file:\n",
    "    uk_ltla_info_dict = json.load(json_file)\n",
    "\n",
    "uk_ltla_info_df = pd.DataFrame([d['attributes'] for d in uk_ltla_info_dict['features']])\n",
    "uk_ltla_info_df = uk_ltla_info_df.rename({'LAU117NM': 'area', 'NUTS318NM': 'NUTS3', 'NUTS118NM': 'region'} ,axis=1)\n",
    "uk_ltla_info_df = uk_ltla_info_df.set_index('area')\n",
    "\n",
    "uk_df = pd.read_csv('../../data/raw_data_w_sources/uk_case_deaths.csv', infer_datetime_format=True)\n",
    "uk_df = uk_df.drop(['areaCode', 'newCasesByPublishDate', 'newDeaths28DaysByPublishDate'], axis=1)\n",
    "uk_df['areaType'] = 'UK'\n",
    "uk_df = uk_df.rename({'areaType': 'country', 'areaName':'area', 'newCasesBySpecimenDate': 'new_cases', 'newDeaths28DaysByDeathDate': 'new_deaths'}, axis=1)\n",
    "uk_df = uk_df.set_index(['area', 'date'])\n",
    "\n",
    "def NUTS3_lookup(ltla):\n",
    "    try:\n",
    "        nuts3 = uk_ltla_info_df.loc[ltla]['NUTS3']\n",
    "    except KeyError:\n",
    "#         print(f'{ltla} missing in my lookup table')\n",
    "        nuts3 = 'unknown'\n",
    "    return nuts3\n",
    "\n",
    "nuts3_uk_df = uk_df.reset_index()\n",
    "nuts3_uk_df['NUTS3'] = nuts3_uk_df['area'].map(NUTS3_lookup)\n",
    "days = nuts3_uk_df['date'].unique()\n",
    "nuts3_regions = nuts3_uk_df['NUTS3'].unique()\n",
    "nuts3_df_list = []\n",
    "nuts3_uk_df_merged = None\n",
    "\n",
    "for nuts3_region in nuts3_regions:\n",
    "    if nuts3_region == 'unknown':\n",
    "        continue\n",
    "    \n",
    "    filtered_df = nuts3_uk_df.loc[nuts3_uk_df['NUTS3'] == nuts3_region]\n",
    "    \n",
    "    case_death_series = filtered_df.groupby('date').sum()\n",
    "    case_death_series['area'] = nuts3_region\n",
    "    \n",
    "    if nuts3_uk_df_merged is None:\n",
    "        nuts3_uk_df_merged = copy.deepcopy(case_death_series)\n",
    "    else:\n",
    "        nuts3_uk_df_merged = nuts3_uk_df_merged.append(case_death_series)\n",
    "    \n",
    "nuts3_uk_df_merged = nuts3_uk_df_merged.reset_index()\n",
    "nuts3_uk_df_merged['date'] = pd.to_datetime(nuts3_uk_df_merged['date'])\n",
    "nuts3_uk_df_merged = nuts3_uk_df_merged.set_index(['area', 'date'])\n",
    "nuts3_uk_df_merged = nuts3_uk_df_merged.sort_index(level=[1],ascending=[True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_cases, uk_deaths = load_new_cases_deaths_from_timeseries_df(uk_Rs, nuts3_uk_df_merged, Ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Austria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently reads directly from the CSV\n",
    "at_df = pd.read_csv('../../data/npi_data/austria.csv', skiprows=2).dropna(axis='index', how='all')#.set_index('Code')\n",
    "at_df = rename_university_col(at_df)\n",
    "droplist = [c for c in at_df.columns if 'Sources' in c or 'Quotes' in c or 'Description' in c or 'What is the reason' in c or 'How many' in c or 'Unnamed' in c]\n",
    "\n",
    "at_df = at_df.drop(droplist, axis=1)\n",
    "at_df = at_df.rename(columns=lambda x: x.strip())\n",
    "\n",
    "at_df = at_df.set_index('Region')\n",
    "\n",
    "at_Rs = ['Wien', 'Burgenland', 'Steiermark', 'Oberösterreich',\n",
    "       'Nieder­österreich', 'Voralberg ', 'Tirol', 'Karnten/Carinthia',\n",
    "       'Salzburg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "at_active_cms = create_active_cms_mat(at_df, at_Rs, npi_information_dict, Ds, final=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austria_ltla_lookup = pd.read_csv('../../data/raw_data_w_sources/at_lau_lookup.csv')\n",
    "austria_ltla_lookup = austria_ltla_lookup.set_index('GKZ')\n",
    "\n",
    "def at_ltla_lookup(ltla):\n",
    "    if ltla in austria_ltla_lookup.index:\n",
    "        return austria_ltla_lookup.loc[ltla]['State Code (middle column of HASC)']\n",
    "    return 'Vienna'\n",
    "\n",
    "austria_df = pd.read_csv('../../data/raw_data_w_sources/at_case_deaths.csv', error_bad_lines=False, delimiter=';', skiprows=1)\n",
    "austria_df = austria_df.drop([' number of cases total',\n",
    "       ' number of cases of 7 days', ' seven days of incidence cases',' number of total totals',\n",
    "       ' number of held daily', ' number of healing total'], axis=1)\n",
    "austria_df[' GKZ'] = austria_df[' GKZ'].map(at_ltla_lookup)\n",
    "austria_df = austria_df.rename({'Time': 'date', ' district': 'area', ' GKZ': 'region', ' number of inhabitants': 'population', ' number of cases': 'new_cases', ' number of dead daily': 'new_deaths'}, axis=1)\n",
    "austria_df = austria_df.drop('population', axis=1)\n",
    "austria_df['date'] = pd.to_datetime(austria_df['date'], format='%d.%m.%Y %M:%H:%S')\n",
    "\n",
    "austria_timeseries_df = austria_df.set_index(['area', 'date'])\n",
    "\n",
    "austria_nuts2_regions = austria_timeseries_df['region'].unique()\n",
    "\n",
    "austria_nuts2_df_list = []\n",
    "\n",
    "austria_nuts2_df_merged = None\n",
    "\n",
    "for nuts2_region in austria_nuts2_regions:    \n",
    "    filtered_df = austria_timeseries_df.loc[austria_timeseries_df['region'] == nuts2_region]\n",
    "    \n",
    "    case_death_series = filtered_df.groupby('date').sum()\n",
    "    case_death_series['area'] = nuts2_region\n",
    "    \n",
    "    if austria_nuts2_df_merged is None:\n",
    "        austria_nuts2_df_merged = copy.deepcopy(case_death_series)\n",
    "    else:\n",
    "        austria_nuts2_df_merged = austria_nuts2_df_merged.append(case_death_series)\n",
    "    \n",
    "austria_nuts2_df_merged = austria_nuts2_df_merged.reset_index()\n",
    "austria_nuts2_df_merged = austria_nuts2_df_merged.set_index(['area', 'date'])\n",
    "austria_nuts2_df_merged = austria_nuts2_df_merged.sort_index(level=[1],ascending=[True])\n",
    "\n",
    "austria_nuts2_timeseries_df = austria_nuts2_df_merged\n",
    "\n",
    "timeseries_df_map_dict = {\n",
    "    'BU': 'Burgenland',\n",
    "    'Vienna': 'Wien',\n",
    "    'ST': 'Steiermark',\n",
    "    'OO': 'Oberösterreich',\n",
    "    'TR': 'Tirol',\n",
    "    'VO': 'Voralberg ',\n",
    "    'KA': 'Karnten/Carinthia',\n",
    "    'OO': 'Oberösterreich',\n",
    "    'NO': 'Nieder\\xadösterreich',\n",
    "    'SZ': 'Salzburg',\n",
    "}\n",
    "\n",
    "austria_nuts2_timeseries_df.index = austria_nuts2_timeseries_df.index.map(lambda x: (timeseries_df_map_dict[x[0]], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "at_cases, at_deaths = load_new_cases_deaths_from_timeseries_df(at_Rs, austria_nuts2_timeseries_df, Ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Germany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently reads directly from the CSV\n",
    "de_df = pd.read_csv('../../data/npi_data/germany.csv', skiprows=2).dropna(axis='index', how='all')#.set_index('Code')\n",
    "de_df = rename_university_col(de_df)\n",
    "droplist = [c for c in de_df.columns if 'Sources' in c or 'Quotes' in c or 'Description' in c or 'What is the reason' in c or 'How many' in c or 'Unnamed' in c]\n",
    "# droplist.extend(['Person who entered this row'])\n",
    "\n",
    "de_df = de_df.drop(droplist, axis=1)\n",
    "de_df = de_df.rename(columns=lambda x: x.strip())\n",
    "de_df['Local area'] = de_df['Local area'].apply(lambda x: str(x).strip())\n",
    "\n",
    "de_Rs = ['Nürnberg', 'LK Aschaffenburg', 'Fürth', 'Landsberg am Lech',\n",
    "       'LK Donau-Ries', 'Minden-Lübbecke', 'Mönchengladbach', 'Münster',\n",
    "       'Rhein-Kreis Neuss', 'LK Ennepe-Ruhr-Kreis', 'LK Rems-Murr-Kreis',\n",
    "       'LK Breisgau-Hochschwarzwald', 'LK Enzkreis', 'LK Hildesheim',\n",
    "       'LK Gifhorn']\n",
    "de_df = de_df.set_index('Local area')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "de_df = no_universities(de_df, de_Rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "de_active_cms = create_active_cms_mat(de_df, de_Rs, npi_information_dict, Ds, final=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ags dict contains information about the local areas of germany\n",
    "with open('../../data/raw_data_w_sources/de_ags.json') as json_file:\n",
    "    ags_info_dict = json.load(json_file)\n",
    "    \n",
    "cases_df = pd.read_csv('../../data/raw_data_w_sources/de_cases-rki-by-ags.csv')\n",
    "cases_df = cases_df.drop('sum_cases', axis=1)\n",
    "cases_df = cases_df.rename({'time_iso8601': 'date'}, axis=1)\n",
    "cases_df['date'] = pd.to_datetime(cases_df['date'])\n",
    "cases_df['date'] = pd.to_datetime(cases_df['date'].dt.date)\n",
    "cases_df = cases_df.set_index('date')\n",
    "cases_df = cases_df.diff()\n",
    "\n",
    "deaths_df = pd.read_csv('../../data/raw_data_w_sources/de_deaths-rki-by-ags.csv')\n",
    "deaths = deaths_df.drop('sum_deaths', axis=1)\n",
    "deaths_df = deaths_df.rename({'time_iso8601': 'date'}, axis=1)\n",
    "deaths_df['date'] = pd.to_datetime(deaths_df['date'])\n",
    "deaths_df['date'] = pd.to_datetime(deaths_df['date'].dt.date)\n",
    "deaths_df = deaths_df.set_index('date')\n",
    "deaths_df = deaths_df.diff()\n",
    "\n",
    "ags_time_series_list = []\n",
    "\n",
    "Ds_germany = pd.date_range('2020-03-02', window_end_date)\n",
    "for ags in ags_info_dict.keys():\n",
    "    if ags == '3152':\n",
    "        continue\n",
    "        \n",
    "    for d in Ds_germany:\n",
    "        ags_dict = {\n",
    "            'area': ags_info_dict[ags]['name'],\n",
    "            'date': d\n",
    "        }\n",
    "        ags_dict['new_cases'] = cases_df[ags][d]\n",
    "        ags_dict['new_deaths'] = deaths_df[ags][d]\n",
    "        \n",
    "        ags_time_series_list.append(ags_dict)\n",
    "\n",
    "germany_timeseries_df = pd.DataFrame(ags_time_series_list)\n",
    "germany_timeseries_df = germany_timeseries_df.set_index(['area', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_Rs_conv = [\n",
    " 'SK Nürnberg',\n",
    " 'LK Aschaffenburg',\n",
    " 'LK Fürth',\n",
    " 'LK Landsberg a.Lech',\n",
    " 'LK Donau-Ries',\n",
    " 'LK Minden-Lübbecke',\n",
    " 'SK Mönchengladbach',\n",
    " 'SK Münster',\n",
    " 'LK Rhein-Kreis Neuss',\n",
    " 'LK Ennepe-Ruhr-Kreis',\n",
    " 'LK Rems-Murr-Kreis',\n",
    " 'LK Breisgau-Hochschwarzwald',\n",
    " 'LK Enzkreis',\n",
    " 'LK Hildesheim',\n",
    " 'LK Gifhorn' \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for cs_name, npi_name in zip(de_Rs_conv, de_Rs):\n",
    "#     print(f'Cases and deaths from {cs_name}, NPIs from {npi_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_cases, de_deaths = load_new_cases_deaths_from_timeseries_df(de_Rs_conv, germany_timeseries_df, Ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Italy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently reads directly from the CSV\n",
    "it_df = pd.read_csv('../../data/npi_data/italy.csv', skiprows=2).dropna(axis='index', how='all')#.set_index('Code')\n",
    "it_df = rename_university_col(it_df)\n",
    "\n",
    "droplist = [c for c in it_df.columns if 'Sources' in c or 'Quotes' in c or 'Description' in c or 'What is the reason' in c or 'How many' in c or 'Unnamed' in c]\n",
    "droplist.append('Local area')\n",
    "\n",
    "it_df = it_df.drop(droplist, axis=1)\n",
    "it_df = it_df.rename(columns=lambda x: x.strip())\n",
    "\n",
    "it_df = it_df.set_index('Region')\n",
    "\n",
    "it_Rs = ['Abruzzo', 'Aosta Valley', 'Apulia (AKA Puglia)','Basilicata','Calabria','Campania','Emilia-Romagna',\n",
    "        'Friuli-Venezia Giulia','Lazio','Liguria ','Lombardy','Marche','Molise','Piedmont','Sardinia','Sicily',\n",
    "         'Trentino (aka Trento)', 'South Tyrol (aka Bolzano aka Alto-Adige)', 'Tuscania ','Umbria','Veneto']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "it_df = no_universities(it_df, it_Rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it_Rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it_Rs = ['Abruzzo', 'Aosta Valley',\n",
    "#                       'Apulia (AKA Puglia)',\n",
    "#                                'Basilicata',\n",
    "#                                  'Calabria',\n",
    "#                                  'Campania',\n",
    "#                            'Emilia-Romagna',\n",
    "#                     'Friuli-Venezia Giulia',\n",
    "#                                     'Lazio',\n",
    "#                                  'Liguria ',\n",
    "#                                  'Lombardy',\n",
    "#                                    'Marche',\n",
    "#                                    'Molise',\n",
    "#                                  'Piedmont',\n",
    "#                                  'Sardinia',\n",
    "#                                    'Sicily',\n",
    "#                      'Trentino-South Tyrol',\n",
    "#                                 'Tuscania ',\n",
    "#                                    'Umbria',\n",
    "#                                    'Veneto']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_active_cms = create_active_cms_mat(it_df, it_Rs, npi_information_dict, Ds, final=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italy_df = pd.read_csv('../../data/raw_data_w_sources/it_cases_deaths.csv', delimiter=',')\n",
    "italy_df['date'] = pd.to_datetime(italy_df['date'])\n",
    "italy_df['date'] = italy_df['date'].dt.date\n",
    "italy_df = italy_df.set_index(['area', 'date'])\n",
    "italy_df['new_deaths'] = italy_df.groupby('area').diff()['total_deaths']\n",
    "italy_df = italy_df.drop('total_deaths', axis=1)\n",
    "\n",
    "italy_timeseries_df = italy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_Rs_conv = ['Abruzzo',\n",
    " \"Valle d'Aosta\",\n",
    " 'Puglia',\n",
    " 'Basilicata',\n",
    " 'Calabria',\n",
    " 'Campania',\n",
    " 'Emilia-Romagna',\n",
    " 'Friuli Venezia Giulia',\n",
    " 'Lazio',\n",
    " 'Liguria',\n",
    " 'Lombardia',\n",
    " 'Marche',\n",
    " 'Molise',\n",
    " 'Piemonte',\n",
    " 'Sardegna',\n",
    " 'Sicilia',\n",
    " 'P.A. Trento',\n",
    " 'P.A. Bolzano', # or P.A. Trento\n",
    " 'Toscana',\n",
    " 'Umbria',\n",
    " 'Veneto']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for cs_name, npi_name in zip(it_Rs_conv, it_Rs):\n",
    "#     print(f'Cases and deaths from {cs_name}, NPIs from {npi_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_cases, it_deaths = load_new_cases_deaths_from_timeseries_df(it_Rs_conv, italy_timeseries_df, Ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Czech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# currently reads directly from the CSV\n",
    "# cz_df = pd.read_csv('../../data/npi_data/czech.csv', skiprows=2).dropna(axis='index', how='all')#.set_index('Code')\n",
    "cz_df = pd.read_csv('../../data/npi_data/czech.csv', skiprows=2).dropna(axis='index', how='all')#.set_index('Code')\n",
    "cz_df = rename_university_col(cz_df)\n",
    "droplist = [c for c in cz_df.columns if 'Sources' in c or 'Quotes' in c or 'Description' in c or 'What is the reason' in c or 'How many' in c or 'Unnamed' in c]\n",
    "\n",
    "\n",
    "cz_df = cz_df.drop(droplist, axis=1)\n",
    "cz_df = cz_df.rename(columns=lambda x: x.strip())\n",
    "cz_df['Region'] = cz_df['Region'].apply(lambda x: str(x).strip())\n",
    "\n",
    "cz_df = cz_df.set_index('Region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cz_Rs = list(cz_df.index.unique())\n",
    "# cz_Rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cz_df = no_universities(cz_df, cz_Rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cz_active_cms = create_active_cms_mat(cz_df, cz_Rs, npi_information_dict, Ds, final=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "czech_df = pd.read_csv('../../data/raw_data_w_sources/cz_cases_deaths.csv', delimiter=',')\n",
    "czech_df = czech_df.rename(columns = {'datum': 'date', 'kraj_nuts_kod':'NUTS3 Unit', 'okres_lau_kod':'LAU Unit', 'kumulativni_pocet_vylecenych':'Recovered', 'kumulativni_pocet_nakazenych': 'Infected', 'kumulativni_pocet_umrti': 'Deaths'})\n",
    "czech_df['date'] = pd.to_datetime(czech_df['date'], format = '%Y-%m-%d')\n",
    "cz_nuts3 = ['CZ042', 'CZ010', 'CZ080', 'CZ020', 'CZ031','CZ063', 'CZ032','CZ041','CZ051', 'CZ052','CZ053', 'CZ071', 'CZ064', 'CZ072']\n",
    "czech_df = czech_df[czech_df['NUTS3 Unit'].isin(cz_nuts3)]\n",
    "cz_nuts3_lookup = {cz_nuts3[i]:cz_Rs[i] for i in range(len(cz_Rs))}\n",
    "czech_df = czech_df.replace({'NUTS3 Unit': cz_nuts3_lookup})\n",
    "czech_df = czech_df.rename(columns = {'NUTS3 Unit':'area'}) #, 'Infected': 'new_cases', 'Deaths': 'new_deaths'})\n",
    "cz_timeseries_df = czech_df.groupby(['area', 'date']).sum()\n",
    "\n",
    "cz_timeseries_df['new_cases'] = cz_timeseries_df.groupby(level=[0]).diff()['Infected']\n",
    "cz_timeseries_df['new_deaths'] = cz_timeseries_df.groupby(level=[0]).diff()['Deaths']\n",
    "\n",
    "# czech_df = czech_df.drop(columns=['LAU Unit', 'Recovered'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "czech_cases, czech_deaths = load_new_cases_deaths_from_timeseries_df(cz_Rs, cz_timeseries_df, Ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Switzerland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ch_df = pd.read_csv('../../data/npi_data/switzerland.csv', skiprows=2).dropna(axis='index', how='all')#.set_index('Code')\n",
    "ch_df = rename_university_col(ch_df)\n",
    "droplist = [c for c in ch_df.columns if 'Sources' in c or 'Quotes' in c or 'Description' in c or 'What is the reason' in c or 'How many' in c or 'Unnamed' in c]\n",
    "\n",
    "\n",
    "ch_df = ch_df.drop(droplist, axis=1)\n",
    "ch_df = ch_df.rename(columns=lambda x: x.strip())\n",
    "ch_df['Region'] = ch_df['Region'].apply(lambda x: str(x).strip())\n",
    "\n",
    "ch_df = ch_df.set_index('Region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_Rs = list(ch_df.index.unique())\n",
    "# cz_Rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ch_df = no_universities(ch_df, ch_Rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ch_active_cms = create_active_cms_mat(ch_df, ch_Rs, npi_information_dict, Ds, final=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "swiss_df = pd.read_csv('../../data/raw_data_w_sources/ch_cases_deaths.csv')\n",
    "swiss_df = swiss_df.drop(swiss_df.columns.difference(['date', 'abbreviation_canton_and_fl', 'ncumul_conf', 'ncumul_deceased']), axis=1)\n",
    "swiss_df = swiss_df.rename({'abbreviation_canton_and_fl': 'area'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../../data/raw_data_w_sources/ch_canton_lookup.json', 'r') as fp:\n",
    "    swiss_canton_lookup = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "swiss_df['date'] = pd.to_datetime(swiss_df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "filled_swiss_df = swiss_df.set_index('date').groupby('area').apply(lambda x: x.reindex(Ds, fill_value=None).drop('area', axis=1)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "swiss_ts_df = filled_swiss_df.replace(swiss_canton_lookup)\n",
    "swiss_ts_df = swiss_ts_df.rename({'level_1': 'date'}, axis=1)\n",
    "swiss_ts_df = swiss_ts_df.set_index(['area', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for canton in swiss_ts_df.index.unique(0):\n",
    "    if np.isnan(swiss_ts_df.loc[canton].loc[Ds[0]]['ncumul_conf']):\n",
    "        swiss_ts_df.loc[(canton, Ds[0]), 'ncumul_conf'] = 0\n",
    "\n",
    "    if np.isnan(swiss_ts_df.loc[canton].loc[Ds[0]]['ncumul_deceased']):\n",
    "        swiss_ts_df.loc[(canton, Ds[0]), 'ncumul_deceased'] = 0\n",
    "\n",
    "    interp_df = swiss_ts_df.loc[canton].interpolate()\n",
    "    interp_df['new_cases'] = interp_df['ncumul_conf'].diff()\n",
    "    interp_df['new_deaths'] = interp_df['ncumul_deceased'].diff()\n",
    "    for date in Ds:\n",
    "        swiss_ts_df.loc[(canton, date), 'ncumul_conf'] = interp_df.loc[date, 'ncumul_conf']\n",
    "        swiss_ts_df.loc[(canton, date), 'ncumul_deceased'] = interp_df.loc[date, 'ncumul_deceased']\n",
    "        swiss_ts_df.loc[(canton, date), 'new_cases'] = np.around(interp_df.loc[date, 'new_cases'])\n",
    "        swiss_ts_df.loc[(canton, date), 'new_deaths'] = np.around(interp_df.loc[date, 'new_deaths'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warning: there was some missing data here\n",
    "\n",
    "e.g., in some areas, some days are missing. To get around this, I did linear interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "swiss_ts_df = swiss_ts_df.drop(['ncumul_conf', 'ncumul_deceased'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for canton in swiss_ts_df.index.unique(0):\n",
    "    if np.isnan(swiss_ts_df.loc[canton].loc[Ds[0]]['new_cases']):\n",
    "        swiss_ts_df.loc[(canton, Ds[0]), 'new_cases'] = 0\n",
    "\n",
    "    if np.isnan(swiss_ts_df.loc[canton].loc[Ds[0]]['new_deaths']):\n",
    "        swiss_ts_df.loc[(canton, Ds[0]), 'new_deaths'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ch_timeseries_df = swiss_ts_df\n",
    "swiss_cases, swiss_deaths = load_new_cases_deaths_from_timeseries_df(ch_Rs, ch_timeseries_df, Ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nl_df = pd.read_csv('../../data/npi_data/netherlands.csv', skiprows=2).dropna(axis='index', how='all')#.set_index('Code')\n",
    "nl_df = rename_university_col(nl_df)\n",
    "droplist = [c for c in nl_df.columns if 'Sources' in c or 'Quotes' in c or 'Description' in c or 'What is the reason' in c or 'How many' in c or 'Unnamed' in c]\n",
    "\n",
    "nl_df = nl_df.drop(droplist, axis=1)\n",
    "nl_df = nl_df.rename(columns=lambda x: x.strip())\n",
    "nl_df['Region'] = nl_df['Region'].apply(lambda x: str(x).strip())\n",
    "\n",
    "nl_df = nl_df.set_index('Region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nl_Rs = list(nl_df.index.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nl_df = no_universities(nl_df, nl_Rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nl_active_cms = create_active_cms_mat(nl_df, nl_Rs, npi_information_dict, Ds, final=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "netherlands_df = pd.read_csv('../../data/raw_data_w_sources/nl_cases_deaths.csv', sep=';')\n",
    "security_regions = list(np.sort(netherlands_df['Security_region_name'].unique()[:-1]))\n",
    "limburg_south = security_regions[15]\n",
    "security_regions.remove(limburg_south)\n",
    "security_regions.append(limburg_south)\n",
    "\n",
    "nl_lookup = {security_regions[i]:nl_Rs[i] for i in range(len(nl_Rs))}\n",
    "netherlands_df = netherlands_df.replace({'Security_region_name': nl_lookup})\n",
    "netherlands_df['area'] = netherlands_df['Security_region_name']\n",
    "netherlands_df['date'] = pd.to_datetime(netherlands_df['Date_of_publication'], format = '%Y-%m-%d')\n",
    "netherlands_df = netherlands_df.groupby(['area', 'date']).sum()\n",
    "netherlands_df = netherlands_df.drop(columns=['Hospital_admission'])\n",
    "netherlands_df = netherlands_df.rename(columns={'Total_reported': 'new_cases', 'Deceased':'new_deaths'})\n",
    "nl_timeseries_df = netherlands_df\n",
    "\n",
    "nl_cases, nl_deaths = load_new_cases_deaths_from_timeseries_df(nl_Rs, nl_timeseries_df, Ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import pylab as pl\n",
    "# \n",
    "# for r, _ in enumerate(nl_Rs):\n",
    "#     fig = plt.figure(figsize=((25,5)))\n",
    "#     ax =  fig.add_subplot(1, 2, 1)\n",
    "#     sns.scatterplot(x=Ds, y=nl_cases[r, :], label = f'Cases', alpha = 0.8, ax = ax)\n",
    "#     ax2 =  fig.add_subplot(1, 2, 2)\n",
    "#     sns.scatterplot(x=Ds, y=nl_deaths[r, :] + np.random.normal(0,0.3,162), label = f'Deaths', alpha = 0.8, ax = ax2)\n",
    "#     pl.suptitle(f'{nl_Rs[r]}')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create merged CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "CMs = list(npi_information_dict.keys())\n",
    "countries = ['England', 'Austria', 'Germany', 'Italy', 'Czech', 'Switzerland', 'Netherlands']\n",
    "new_cases = [uk_cases, at_cases, de_cases, it_cases, czech_cases, swiss_cases, nl_cases]\n",
    "new_deaths = [uk_deaths, at_deaths, de_deaths, it_deaths, czech_deaths, swiss_deaths, nl_deaths]\n",
    "active_cms = [uk_active_cms, at_active_cms, de_active_cms, it_active_cms, cz_active_cms, ch_active_cms, nl_active_cms]\n",
    "Rs = [uk_Rs, at_Rs, de_Rs, it_Rs, cz_Rs, ch_Rs, nl_Rs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print(Rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def set_household_limits(active_CMs, household_NPI_index, gathering_NPI_index):\n",
    "    nRs, _, nDs = active_CMs.shape\n",
    "    new_acms = np.copy(active_CMs)\n",
    "    for r in range(nRs):\n",
    "        for day in range(nDs):\n",
    "            if active_CMs[r, household_NPI_index, day] == 0 or active_CMs[r, gathering_NPI_index, day] < active_CMs[r, household_NPI_index, day]:\n",
    "                new_acms[r, household_NPI_index, day] = active_CMs[r, gathering_NPI_index, day]\n",
    "    return new_acms\n",
    "\n",
    "def all_gathering_bans(active_CMs, household_NPI_index, gathering_NPI_index):\n",
    "    nRs, _, nDs = active_CMs.shape\n",
    "    new_acms = np.copy(active_CMs)\n",
    "    for r in range(nRs):\n",
    "        for day in range(nDs):\n",
    "            if int(active_CMs[r, household_NPI_index, day]) == 1 and active_CMs[r, gathering_NPI_index, day] == 0:\n",
    "                print('mode 3 in use')\n",
    "                new_acms[r, gathering_NPI_index, day] = 1\n",
    "    return new_acms\n",
    "\n",
    "def set_all_limits(active_CMs, fun):\n",
    "    new_acms = np.copy(active_CMs)\n",
    "    new_acms = fun(new_acms, 4, 0)\n",
    "    new_acms = fun(new_acms, 5, 1)\n",
    "    new_acms = fun(new_acms, 6, 2)\n",
    "    new_acms = fun(new_acms, 7, 3)\n",
    "    return new_acms\n",
    "\n",
    "active_cms = [set_all_limits(active_cm, set_household_limits) for active_cm in active_cms]\n",
    "active_cms = [set_all_limits(active_cm, all_gathering_bans) for active_cm in active_cms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_rows = []\n",
    " \n",
    "for c, ncs, nds, acms, rs in zip(countries, new_cases, new_deaths, active_cms, Rs):\n",
    "    # print(c)\n",
    "    for r_i, r in enumerate(rs):\n",
    "        for d_i, d in enumerate(Ds):\n",
    "            row_dict = {\n",
    "                'Country': c,\n",
    "                'Area': r,\n",
    "                'Date': d,\n",
    "                'New Cases': ncs[r_i, d_i],\n",
    "                'New Deaths': nds[r_i, d_i]\n",
    "            }\n",
    "\n",
    "            for cm_i, cm in enumerate(CMs):\n",
    "                row_dict[cm] = acms[r_i, cm_i, d_i]\n",
    "\n",
    "            all_rows.append(row_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.DataFrame(all_rows).set_index(['Area', 'Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "merged_df.to_csv(f'../../data/all_merged_data_{window_end_date}_t.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Sanity Checks\n",
    "\n",
    "Some of the errors below may be solved by fixing the errors presented in the reports above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def some_businesses(active_CMs, Rs, Ds):\n",
    "    # Any of the businesses NPIs (except “some businesses”) can only be active when “some businesses” is active.\n",
    "    nRs, _, nDs = active_CMs.shape\n",
    "    some_business_ind = 9\n",
    "    business_inds = [10, 11, 12, 13]\n",
    "    for r in range(nRs):\n",
    "        for day in range(nDs):\n",
    "            for ind in business_inds:\n",
    "                if active_CMs[r, ind, day] != 0 and active_CMs[r, some_business_ind, day] == 0:\n",
    "                    print(f'Some f2f business not active but {CMs[ind]} is active in {Rs[r]} on {Ds[day]}')\n",
    "\n",
    "def all_nonessential_businesses(active_CMs, Rs, Ds):\n",
    "    #“All nonessential businesses” can only be active when all other business NPIs are active\n",
    "    nRs, _, nDs = active_CMs.shape\n",
    "    all_business_ind = 13\n",
    "    business_inds = [9, 10, 11, 12]\n",
    "    for r in range(nRs):\n",
    "        for day in range(nDs):\n",
    "            for ind in business_inds:\n",
    "                if active_CMs[r, ind, day] == 0 and active_CMs[r, all_business_ind, day] == 1:\n",
    "                    print(f'All non-essential business is active but {CMs[ind]} is not active in {Rs[r]} on {Ds[day]}')\n",
    "\n",
    "def curfew_and_stay_home(active_CMs, Rs, Ds):\n",
    "    #“All nonessential businesses” can only be active when all other business NPIs are active\n",
    "    nRs, _, nDs = active_CMs.shape\n",
    "    curfew_ind = 15\n",
    "    stay_home_ind = 14\n",
    "    for r in range(nRs):\n",
    "        for day in range(nDs):\n",
    "            if active_CMs[r, curfew_ind, day] == 1 and active_CMs[r, stay_home_ind, day] == 1:\n",
    "                print(f'Both Curfew and Stay-at-home order active in {Rs[r]} on {Ds[day]}')\n",
    "\n",
    "def testing_household_gathering(active_CMs, Rs, Ds):\n",
    "    #“All nonessential businesses” can only be active when all other business NPIs are active\n",
    "    nRs, _, nDs = active_CMs.shape\n",
    "    pair1 = [0, 4]\n",
    "    pair2 = [1, 5]\n",
    "    pair3 = [2, 6]\n",
    "    pair4 = [3, 7]\n",
    "    pairs = [pair1, pair2, pair3, pair4]\n",
    "    for r in range(nRs):\n",
    "        for day in range(nDs):\n",
    "            for pair in pairs:\n",
    "                if active_CMs[r, pair[1], day] > active_CMs[r, pair[0], day] or active_CMs[r, pair[1], day] == 0:\n",
    "                    if active_CMs[r, pair[0], day] != 0:\n",
    "                        print(f'The {CMs[pair[0]]} is stricter than the {CMs[pair[1]]} in {Rs[r]} on {Ds[day]}')\n",
    "\n",
    "def get_short_gaps_region(active_CMs, area_indx, Rs, Ds, threshold):\n",
    "    CMs_data = (active_CMs[area_indx, :, :] > 0).astype(int)\n",
    "    nCMs, _ = CMs_data.shape\n",
    "    CM_changes = np.zeros((nCMs, len(Ds)))\n",
    "    CM_changes[:, 1:] = CMs_data[:, 1:] - CMs_data[:, :-1]\n",
    "    CM_changes = (CM_changes != 0).astype(int)\n",
    "    for i in range(nCMs):\n",
    "        for j in range(len(Ds)-threshold):\n",
    "            if sum(CM_changes[i, j:j+threshold]) > 1:\n",
    "                print(f'{CMs[i]} switched its activation status more than once within a period of {threshold} days in {Rs[area_indx]} on {Ds[j]}')\n",
    "\n",
    "def get_short_gaps(active_CMs, Rs, Ds, threshold):\n",
    "    for i in range(len(Rs)):\n",
    "        get_short_gaps_region(active_CMs, i, Rs, Ds, threshold)\n",
    "\n",
    "def plot_timeline_value_region(active_CMs, area_indx, Rs, Ds):\n",
    "    fig = plt.figure(figsize=((15,10)))\n",
    "    for i in range(4):\n",
    "        ax = fig.add_subplot(2, 2, i+1)\n",
    "        CM_gathering_data = active_CMs[area_indx, i, :]\n",
    "        sns.lineplot(x=Ds, y=list(CM_gathering_data), label = f'{CMs[i]}', ax = ax)\n",
    "        CM_household_data = active_CMs[area_indx, i+4, :]\n",
    "        sns.lineplot(x=Ds, y=list(CM_household_data), label = f'{CMs[i+4]}', ax = ax)\n",
    "        plt.title(f'{CMs[i]} and {CMs[i+4]}')\n",
    "    pl.suptitle(f'{Rs[area_indx]}: Timeline plots for value NPIs')\n",
    "    plt.show()\n",
    "\n",
    "def plot_timeline_value(active_CMs, Rs, Ds):\n",
    "    for R in range(len(Rs)):\n",
    "        plot_timeline_value_region(active_CMs, R, Rs, Ds)\n",
    "\n",
    "def perform_sanity_checks(country, Rs, Ds, threshold, plots=False):\n",
    "    index = countries.index(country)\n",
    "    active_CMs = active_cms[index]\n",
    "\n",
    "    some_businesses(active_CMs, Rs, Ds)\n",
    "    all_nonessential_businesses(active_CMs, Rs, Ds)\n",
    "    curfew_and_stay_home(active_CMs, Rs, Ds)\n",
    "    testing_household_gathering(active_CMs, Rs, Ds)\n",
    "    get_short_gaps(active_CMs, Rs, Ds, threshold)\n",
    "    if plots==True:\n",
    "        plot_timeline_value(active_CMs, Rs, Ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## England"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "perform_sanity_checks('England', uk_Rs, Ds, 3, plots=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Austria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "perform_sanity_checks('Austria', at_Rs, Ds, 3, plots=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Germany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "perform_sanity_checks('Germany', de_Rs, Ds, 3, plots=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Italy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "perform_sanity_checks('Italy', it_Rs, Ds, 3, plots=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Czech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "perform_sanity_checks('Czech', cz_Rs, Ds, 3, plots=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switzerland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "perform_sanity_checks('Switzerland', ch_Rs, Ds, 3, plots=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "perform_sanity_checks('Netherlands', nl_Rs, Ds, 3, plots=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Investigating differences in gathering and household limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_timeline_diffs_region(active_CMs, area_indx, Rs, Ds):\n",
    "    for i in range(4):\n",
    "        CM_gathering_data = active_CMs[area_indx, i, :]\n",
    "        CM_household_data = active_CMs[area_indx, i+4, :]\n",
    "        if list(CM_gathering_data) != list(CM_household_data):\n",
    "            equal_rows = [i for i in range(len(list(CM_gathering_data))) if list(CM_gathering_data)[i] == list(CM_household_data)[i]]\n",
    "            CM_gathering_data[equal_rows] = [None for i in range(len(equal_rows))]\n",
    "            CM_household_data[equal_rows] = [None for i in range(len(equal_rows))]\n",
    "            sns.lineplot(x=Ds, y=list(CM_gathering_data), label = f'{CMs[i]}')\n",
    "            sns.lineplot(x=Ds, y=list(CM_household_data), label = f'{CMs[i+4]}')\n",
    "            plt.title(f'{Rs[area_indx]}')\n",
    "            plt.show()\n",
    "\n",
    "def plot_timeline_diffs(active_CMs, Rs, Ds):\n",
    "    for R in range(len(Rs)):\n",
    "        plot_timeline_diffs_region(active_CMs, R, Rs, Ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plot_timeline_diffs(active_cms[0], uk_Rs, Ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plot_timeline_diffs(active_cms[1], at_Rs, Ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plot_timeline_diffs(active_cms[2], de_Rs, Ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plot_timeline_diffs(active_cms[3], it_Rs, Ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plot_timeline_diffs(active_cms[4], cz_Rs, Ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plot_timeline_diffs(active_cms[5], ch_Rs, Ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plot_timeline_diffs(active_cms[6], nl_Rs, Ds)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": [
     "\n"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
